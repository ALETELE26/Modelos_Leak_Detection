{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWy8d-iwOcyi"
      },
      "source": [
        "# Importando los modulos necesarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwEHLifdl34R",
        "outputId": "610bda35-ed88-4ae3-b6cc-3fb6ee046fc8"
      },
      "outputs": [],
      "source": [
        "# Instalación de dependencias\n",
        "!pip install qkeras keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0NQ9qejK1UJ"
      },
      "outputs": [],
      "source": [
        "    import h5py\n",
        "    import numpy as np\n",
        "    import json\n",
        "    import tensorflow as tf\n",
        "    tf.config.run_functions_eagerly(True)\n",
        "    tf.data.experimental.enable_debug_mode()\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "    from tensorflow.keras.layers import Input, MaxPooling2D, Flatten, Dropout, Add\n",
        "    from tensorflow.keras.models import Model\n",
        "    import matplotlib.pyplot as plt\n",
        "    from qkeras import *\n",
        "    import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0FAYi3SQCgi"
      },
      "source": [
        "#Cargando el dataset de escalogramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfPdEQKQQOY",
        "outputId": "a7667b1d-5cbd-412d-c4fa-813fa9b57866"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/accelerometer_BR_256NS_20Scales_cwt_dataset.h5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QCaZ9PdOwpk"
      },
      "outputs": [],
      "source": [
        "def load_cwt_dataset(file_path='accelerometer_cwt_dataset.h5'):\n",
        "    \"\"\"\n",
        "    Carga el dataset de escalogramas desde un archivo HDF5 y lo prepara para Keras.\n",
        "\n",
        "    Args:\n",
        "        file_path: Ruta al archivo HDF5\n",
        "\n",
        "    Returns:\n",
        "        dataset: Un diccionario con los datos y metadatos\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    with h5py.File(file_path, 'r') as hf:\n",
        "        # Cargar escalogramas y etiquetas\n",
        "        x_train = np.array(hf['train']['scalograms'])\n",
        "        y_train = np.array(hf['train']['labels'])\n",
        "        x_test = np.array(hf['test']['scalograms'])\n",
        "        y_test = np.array(hf['test']['labels'])\n",
        "\n",
        "        # Cargar metadatos\n",
        "        num_classes = hf['metadata']['num_classes'][()]\n",
        "        shape = tuple(hf['metadata']['shape'][()])\n",
        "\n",
        "        # Cargar diccionario de etiquetas\n",
        "        label_codes_dict = json.loads(hf['metadata'].attrs['label_codes_dict'])\n",
        "\n",
        "    # Organizar datos como un dataset tipo Keras\n",
        "    dataset = {\n",
        "        'train': (x_train, y_train),\n",
        "        'test': (x_test, y_test),\n",
        "        'num_classes': num_classes,\n",
        "        'input_shape': shape,\n",
        "        'label_codes_dict': label_codes_dict\n",
        "    }\n",
        "\n",
        "    print(f\"Dataset cargado: {x_train.shape[0]} muestras de entrenamiento, {x_test.shape[0]} muestras de prueba\")\n",
        "    print(f\"Forma de cada escalograma: {shape}\")\n",
        "    print(f\"Número de clases: {num_classes}\")\n",
        "\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajCrqRAcSP9k",
        "outputId": "b9432133-e549-4bc5-dfa3-03763c109f04"
      },
      "outputs": [],
      "source": [
        "# Carga el dataset\n",
        "dataset = load_cwt_dataset(file_path)\n",
        "\n",
        "# Obtén las partes del dataset\n",
        "x_train, y_train = dataset['train']\n",
        "x_test, y_test = dataset['test']\n",
        "input_shape = dataset['input_shape']\n",
        "num_classes = dataset['num_classes']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVbk7WdRYJqD"
      },
      "source": [
        "#Preparar las etiquetas con one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liggasglYS68",
        "outputId": "b511d225-4fa6-4747-ac37-b7850576a40c"
      },
      "outputs": [],
      "source": [
        "# Convertir etiquetas a formato one-hot\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Verificar la forma actual de las etiquetas\n",
        "print(\"Forma original de y_train:\", y_train.shape)\n",
        "print(\"Primeros 5 valores de y_train:\", y_train[:5])\n",
        "\n",
        "# Convertir etiquetas a formato one-hot\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Verificar la nueva forma\n",
        "print(\"Nueva forma de y_train:\", y_train.shape)\n",
        "print(\"Primeros 5 valores de y_train:\", y_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0-d98e7aDvU"
      },
      "source": [
        "#Verificando la normalización de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "jHGpULI8aK3K",
        "outputId": "3da30506-583f-4a71-9941-5689f5ded556"
      },
      "outputs": [],
      "source": [
        "# Verificar si los datos ya están normalizados\n",
        "print(\"Estado actual de los datos:\")\n",
        "print(\"Valor máximo en x_train:\", np.max(x_train))\n",
        "print(\"Valor mínimo en x_train:\", np.min(x_train))\n",
        "print(\"Media de x_train:\", np.mean(x_train))\n",
        "print(\"Desviación estándar de x_train:\", np.std(x_train))\n",
        "\n",
        "# Visualizar la distribución de valores\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(x_train.flatten(), bins=50)\n",
        "plt.title('Distribución de valores en x_train')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sounit5YbD9m"
      },
      "source": [
        "#No estan normalizado, a normalizar!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW311Ut4bJ6C",
        "outputId": "0065d6e3-d5d4-464b-eaf7-d13c4a2fefe1"
      },
      "outputs": [],
      "source": [
        "# Normalizar los datos de entrada usando el máximo de x_train para ambos conjuntos\n",
        "max_value = np.max(x_train)  # Usar sólo el conjunto de entrenamiento para calcular el valor\n",
        "\n",
        "# Convertir a float32 y normalizar\n",
        "x_train = x_train.astype('float32') / max_value\n",
        "x_test = x_test.astype('float32') / max_value  # Usar el mismo valor para test\n",
        "\n",
        "print(\"Después de normalizar:\")\n",
        "print(\"Valor máximo en x_train:\", np.max(x_train))\n",
        "print(\"Valor mínimo en x_train:\", np.min(x_train))\n",
        "print(\"Valor máximo en x_test:\", np.max(x_test))\n",
        "print(\"Valor mínimo en x_test:\", np.min(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQnhPxKrcfjq"
      },
      "source": [
        "#Adaptando los datos para el modelo CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig1FtbvWcCWS",
        "outputId": "d6d380b5-ba11-4290-9bbc-a12111ab2b1b"
      },
      "outputs": [],
      "source": [
        "print(\"Forma de x_train:\", x_train.shape)\n",
        "print(\"Forma de y_train:\", y_train.shape)\n",
        "print(\"Forma de x_test:\", x_test.shape)\n",
        "print(\"Forma de y_test:\", y_test.shape)\n",
        "# Adaptar dimensiones para CNN (necesitamos un canal)\n",
        "# Asumiendo que las dimensiones actuales son (n_samples, scales, time_steps)\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "\n",
        "print(\"Forma final de x_train:\", x_train.shape)\n",
        "print(\"Forma final de x_test:\", x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZbtscxMdrkW"
      },
      "source": [
        "#Separamos una parte de los datos para la validacion del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHz0TrT6dz_N",
        "outputId": "29b94848-a427-4043-8cbc-15be9706e739"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir los datos\n",
        "x_train_final, x_val, y_train_final, y_val = train_test_split(\n",
        "    x_train, y_train,\n",
        "    test_size=0.2,  # 20% para validación\n",
        "    random_state=42,  # Para reproducibilidad\n",
        "    stratify=y_train  # Mantener la distribución de clases\n",
        ")\n",
        "\n",
        "# Verificar tamaños\n",
        "print(f\"Datos de entrenamiento: {x_train_final.shape[0]} muestras\")\n",
        "print(f\"Datos de validación: {x_val.shape[0]} muestras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyc4e56ZrmTt"
      },
      "source": [
        "# Definiendo los bloques convolucionales de mi modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdsoqRqUrtKi"
      },
      "outputs": [],
      "source": [
        "# Habilitar ejecución eager para TensorFlow\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, MaxPooling2D, Flatten, Dropout, Add, Activation\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Usar el adaptador corregido\n",
        "from bnn_adapter_fixed import binary_activation, binary_conv, binary_dense, my_flat\n",
        "from bnn_adapter_fixed import l1_batch_norm_mod_conv, l1_batch_norm_mod_dense\n",
        "\n",
        "def bnn_model_builder(hp):\n",
        "    \"\"\"\n",
        "    Constructor de modelo BNN para búsqueda de arquitectura.\n",
        "    Combina enfoque de NAS con capas binarias optimizadas para microcontroladores.\n",
        "    \"\"\"\n",
        "    # Hiperparámetros para búsqueda\n",
        "    conv_stages = hp.Int('conv_stages', min_value=1, max_value=3, step=1)\n",
        "    initial_filters = hp.Choice('initial_filters', values=[16, 32, 64])\n",
        "    use_residual = hp.Boolean('use_residual')\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.3, step=0.1)\n",
        "\n",
        "    # Configuración para batch norm\n",
        "    batch_norm_momentum = hp.Float('batch_norm_momentum',\n",
        "                                  min_value=0.8, max_value=0.95, step=0.05)\n",
        "\n",
        "    # Entrada: escalogramas de señales vibroacústicas\n",
        "    inputs = Input(shape=(x_train_final.shape[1], x_train_final.shape[2], 1))\n",
        "\n",
        "    # Primera capa - siempre usar mayor precisión en primera capa\n",
        "    # Usamos una convolución estándar (no binaria) para preservar información de entrada\n",
        "    # Esto sigue las recomendaciones para STM32 y el paper de Wang\n",
        "    x = tf.keras.layers.Conv2D(filters=initial_filters,\n",
        "                               kernel_size=3,\n",
        "                               padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=batch_norm_momentum)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    # Bloques de convolución binarios\n",
        "    filters = initial_filters\n",
        "\n",
        "    for i in range(conv_stages):\n",
        "        # Más bloques en etapas posteriores como en EfficientNetV2\n",
        "        num_blocks = 1 + i\n",
        "\n",
        "        for j in range(num_blocks):\n",
        "            # Guardar entrada para conexión residual\n",
        "            res_input = x\n",
        "\n",
        "            # Convolución binaria con backprop personalizada (del paper)\n",
        "            x = binary_conv(\n",
        "                nfilters=filters,\n",
        "                ch_in=int(x.shape[-1]),\n",
        "                k=3,\n",
        "                padding='same'\n",
        "            )(x)\n",
        "\n",
        "            # MaxPool antes de BatchNorm (recomendación STM32)\n",
        "            if j == num_blocks - 1 and i < conv_stages - 1:  # Al final de cada etapa\n",
        "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "            # BatchNorm L1 modificado (del paper)\n",
        "            x = l1_batch_norm_mod_conv(\n",
        "                batch_size=64,  # Podría ser hiperparámetro\n",
        "                width_in=x.shape[1],\n",
        "                ch_in=filters,\n",
        "                momentum=batch_norm_momentum\n",
        "            )(x)\n",
        "\n",
        "            # Activación binaria\n",
        "            x = binary_activation()(x)\n",
        "\n",
        "            # Conexión residual cuando sea posible\n",
        "            if use_residual and j > 0 and res_input.shape == x.shape:\n",
        "                x = Add()([res_input, x])\n",
        "\n",
        "            # Dropout\n",
        "            if dropout_rate > 0:\n",
        "                x = Dropout(dropout_rate)(x)\n",
        "\n",
        "        # Aumentar filtros para siguiente etapa\n",
        "        filters *= 2\n",
        "\n",
        "    # Aplanar\n",
        "    x = my_flat()(x)\n",
        "\n",
        "    # Capas densas binarias\n",
        "    for units in [256, 128]:\n",
        "        x = binary_dense(\n",
        "            n_in=int(x.shape[-1]),\n",
        "            n_out=units\n",
        "        )(x)\n",
        "        x = l1_batch_norm_mod_dense(\n",
        "            batch_size=64,  # Podría ser hiperparámetro\n",
        "            ch_in=units,\n",
        "            momentum=batch_norm_momentum\n",
        "        )(x)\n",
        "        x = binary_activation()(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Capa de salida - no binaria para mejor precisión\n",
        "    x = tf.keras.layers.Dense(5)(x)  # 5 clases\n",
        "    outputs = Activation('softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Optimizador personalizado para BNN\n",
        "    lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5LFHHMfyIzg"
      },
      "source": [
        "#Función NAS con Keras Tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsS7v2PL0L5k"
      },
      "outputs": [],
      "source": [
        "def bnn_model_builder(hp):\n",
        "    \"\"\"\n",
        "    Constructor de modelo BNN para búsqueda de arquitectura.\n",
        "    Combina enfoque de NAS con capas binarias optimizadas para microcontroladores.\n",
        "    \"\"\"\n",
        "    # Hiperparámetros para búsqueda\n",
        "    conv_stages = hp.Int('conv_stages', min_value=1, max_value=3, step=1)\n",
        "    initial_filters = hp.Choice('initial_filters', values=[16, 32, 64])\n",
        "    use_residual = hp.Boolean('use_residual')\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.3, step=0.1)\n",
        "\n",
        "    # Configuración para batch norm\n",
        "    batch_norm_momentum = hp.Float('batch_norm_momentum',\n",
        "                                  min_value=0.8, max_value=0.95, step=0.05)\n",
        "\n",
        "    # Entrada: escalogramas de señales vibroacústicas\n",
        "    inputs = Input(shape=(x_train_final.shape[1], x_train_final.shape[2], 1))\n",
        "\n",
        "    # Primera capa - siempre usar mayor precisión en primera capa\n",
        "    # Usamos una convolución estándar (no binaria) para preservar información de entrada\n",
        "    # Esto sigue las recomendaciones para STM32 y el paper de Wang\n",
        "    x = tf.keras.layers.Conv2D(filters=initial_filters,\n",
        "                               kernel_size=3,\n",
        "                               padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=batch_norm_momentum)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    # Bloques de convolución binarios\n",
        "    filters = initial_filters\n",
        "\n",
        "    for i in range(conv_stages):\n",
        "        # Más bloques en etapas posteriores como en EfficientNetV2\n",
        "        num_blocks = 1 + i\n",
        "\n",
        "        for j in range(num_blocks):\n",
        "            # Guardar entrada para conexión residual\n",
        "            res_input = x\n",
        "\n",
        "            # Convolución binaria con backprop personalizada (del paper)\n",
        "            x = binary_conv(\n",
        "                nfilters=filters,\n",
        "                ch_in=int(x.shape[-1]),\n",
        "                k=3,\n",
        "                padding='same'\n",
        "            )(x)\n",
        "\n",
        "            # MaxPool antes de BatchNorm (recomendación STM32)\n",
        "            if j == num_blocks - 1 and i < conv_stages - 1:  # Al final de cada etapa\n",
        "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "            # BatchNorm L1 modificado (del paper)\n",
        "            x = l1_batch_norm_mod_conv(\n",
        "                batch_size=64,  # Podría ser hiperparámetro\n",
        "                width_in=x.shape[1],\n",
        "                ch_in=filters,\n",
        "                momentum=batch_norm_momentum\n",
        "            )(x)\n",
        "\n",
        "            # Activación binaria\n",
        "            x = binary_activation()(x)\n",
        "\n",
        "            # Conexión residual cuando sea posible\n",
        "            if use_residual and j > 0 and res_input.shape == x.shape:\n",
        "                x = Add()([res_input, x])\n",
        "\n",
        "            # Dropout\n",
        "            if dropout_rate > 0:\n",
        "                x = Dropout(dropout_rate)(x)\n",
        "\n",
        "        # Aumentar filtros para siguiente etapa\n",
        "        filters *= 2\n",
        "\n",
        "    # Aplanar\n",
        "    x = my_flat()(x)\n",
        "\n",
        "    # Capas densas binarias\n",
        "    for units in [256, 128]:\n",
        "        x = binary_dense(\n",
        "            n_in=int(x.shape[-1]),\n",
        "            n_out=units\n",
        "        )(x)\n",
        "        x = l1_batch_norm_mod_dense(\n",
        "            batch_size=64,  # Podría ser hiperparámetro\n",
        "            ch_in=units,\n",
        "            momentum=batch_norm_momentum\n",
        "        )(x)\n",
        "        x = binary_activation()(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Capa de salida - no binaria para mejor precisión\n",
        "    x = tf.keras.layers.Dense(5)(x)  # 5 clases\n",
        "    outputs = Activation('softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Optimizador personalizado para BNN\n",
        "    lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_2l9Xkw3_h-"
      },
      "source": [
        "# Función de Estimación de Recursos Mejorada para BNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXgr9Ysy4KYg"
      },
      "outputs": [],
      "source": [
        "def estimate_bnn_resources(model):\n",
        "    \"\"\"\n",
        "    Estima uso de recursos para un modelo BNN en STM32L432KC.\n",
        "    Las BNN son mucho más eficientes que los modelos cuantizados estándar.\n",
        "    \"\"\"\n",
        "    # Contar capas binarias\n",
        "    total_params = model.count_params()\n",
        "    binary_params = 0\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, binary_conv) or isinstance(layer, binary_dense):\n",
        "            # Contar parámetros en capas binarias\n",
        "            binary_params += layer.count_params()\n",
        "\n",
        "    # En BNN, cada peso ocupa solo 1 bit en lugar de 8\n",
        "    effective_param_size = (total_params - binary_params) + (binary_params / 8)\n",
        "\n",
        "    # Estimar memoria RAM (más preciso para BNN)\n",
        "    # Las activaciones binarias también usan menos memoria\n",
        "    activation_memory_kb = 5  # Estimación base\n",
        "\n",
        "    # Memoria para parámetros\n",
        "    param_memory_kb = effective_param_size / 8 / 1024\n",
        "\n",
        "    # Memoria de trabajo\n",
        "    working_memory_kb = 10\n",
        "\n",
        "    total_memory_kb = param_memory_kb + activation_memory_kb + working_memory_kb\n",
        "    flash_usage_kb = effective_param_size / 8 / 1024 + 50  # Código base ~50KB\n",
        "\n",
        "    return {\n",
        "        'total_params': total_params,\n",
        "        'binary_params': binary_params,\n",
        "        'effective_param_size': effective_param_size,\n",
        "        'ram_usage_kb': total_memory_kb,\n",
        "        'flash_usage_kb': flash_usage_kb\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvSsrL0e4P5N"
      },
      "source": [
        "# Búsqueda de Arquitectura Multi-Objetivo para BNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLOGvCGv4a9L"
      },
      "outputs": [],
      "source": [
        "class BNNTunerMultiObjective(kt.BayesianOptimization):\n",
        "    def __init__(self, *args,\n",
        "                 accuracy_weight=0.6,\n",
        "                 size_weight=0.2,\n",
        "                 ram_weight=0.2,\n",
        "                 max_ram_kb=48,\n",
        "                 **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.accuracy_weight = accuracy_weight\n",
        "        self.size_weight = size_weight\n",
        "        self.ram_weight = ram_weight\n",
        "        self.max_ram_kb = max_ram_kb\n",
        "\n",
        "    def run_trial(self, trial, *args, **kwargs):\n",
        "        hp = trial.hyperparameters\n",
        "        model = self.hypermodel.build(hp)\n",
        "\n",
        "        # Estimar recursos\n",
        "        resources = estimate_bnn_resources(model)\n",
        "        print(f\"Modelo con {resources['total_params']:,} parámetros\")\n",
        "        print(f\"- Parámetros binarios: {resources['binary_params']:,}\")\n",
        "        print(f\"- RAM estimada: {resources['ram_usage_kb']:.1f} KB\")\n",
        "        print(f\"- Flash estimado: {resources['flash_usage_kb']:.1f} KB\")\n",
        "\n",
        "        # Entrenar modelo\n",
        "        results = super().run_trial(trial, *args, **kwargs)\n",
        "\n",
        "        if results[\"status\"] != kt.engine.trial.TrialStatus.OK:\n",
        "            # Si hay errores, devolver resultados sin score\n",
        "            return results\n",
        "\n",
        "        # Calcular puntuación compuesta\n",
        "        val_accuracy = results[\"metrics\"][\"val_accuracy\"]\n",
        "\n",
        "        # Normalizar puntuaciones (mayor es mejor)\n",
        "        accuracy_score = val_accuracy\n",
        "        size_score = max(0, 1.0 - (resources['flash_usage_kb'] / 256))\n",
        "        ram_score = max(0, 1.0 - (resources['ram_usage_kb'] / self.max_ram_kb))\n",
        "\n",
        "        # Score final ponderado\n",
        "        composite_score = (\n",
        "            self.accuracy_weight * accuracy_score +\n",
        "            self.size_weight * size_score +\n",
        "            self.ram_weight * ram_score\n",
        "        )\n",
        "\n",
        "        # Mostrar resultados\n",
        "        print(f\"\\nModelo evaluado:\")\n",
        "        print(f\"- Precisión: {val_accuracy:.4f}\")\n",
        "        print(f\"- Score tamaño: {size_score:.4f}\")\n",
        "        print(f\"- Score RAM: {ram_score:.4f}\")\n",
        "        print(f\"- Puntuación final: {composite_score:.4f}\\n\")\n",
        "\n",
        "        # Actualizar score\n",
        "        results[\"score\"] = composite_score\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cOGw1Vi5hGu"
      },
      "source": [
        "# Función Principal para Ejecutar la Búsqueda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn5icfPf5yVc"
      },
      "outputs": [],
      "source": [
        "def run_bnn_nas():\n",
        "    \"\"\"\n",
        "    Ejecuta la búsqueda de arquitectura para BNN optimizada para STM32\n",
        "    \"\"\"\n",
        "    print(\"Iniciando búsqueda de arquitectura para BNN...\")\n",
        "\n",
        "    # Instanciar el tuner personalizado\n",
        "    tuner = BNNTunerMultiObjective(\n",
        "        bnn_model_builder,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=20,\n",
        "        directory='nas_results',\n",
        "        project_name='bnn_escalogramas_stm32',\n",
        "        accuracy_weight=0.6,\n",
        "        size_weight=0.2,\n",
        "        ram_weight=0.2,\n",
        "        max_ram_kb=48\n",
        "    )\n",
        "\n",
        "    # Callbacks para entrenamiento\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
        "                                         restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "    ]\n",
        "\n",
        "    # Realizar la búsqueda\n",
        "    tuner.search(\n",
        "        x_train_final, y_train_final,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=30,  # Menos épocas para la búsqueda inicial\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Obtener mejor modelo\n",
        "    best_model = tuner.get_best_models(1)[0]\n",
        "    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "    # Mostrar configuración óptima\n",
        "    print(\"\\n=== MEJOR ARQUITECTURA BNN ===\")\n",
        "    print(f\"Etapas convolucionales: {best_hps.get('conv_stages')}\")\n",
        "    print(f\"Filtros iniciales: {best_hps.get('initial_filters')}\")\n",
        "    print(f\"Usar residual: {best_hps.get('use_residual')}\")\n",
        "    print(f\"Dropout: {best_hps.get('dropout_rate')}\")\n",
        "    print(f\"Momentum BatchNorm: {best_hps.get('batch_norm_momentum')}\")\n",
        "    print(f\"Tasa de aprendizaje: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "    # Estimar recursos del mejor modelo\n",
        "    resources = estimate_bnn_resources(best_model)\n",
        "    print(f\"Parámetros totales: {resources['total_params']:,}\")\n",
        "    print(f\"Parámetros binarios: {resources['binary_params']:,}\")\n",
        "    print(f\"Tamaño efectivo: {resources['effective_param_size']:,}\")\n",
        "    print(f\"RAM estimada: {resources['ram_usage_kb']:.1f} KB\")\n",
        "    print(f\"Flash estimado: {resources['flash_usage_kb']:.1f} KB\")\n",
        "\n",
        "    return best_model, best_hps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoSmcUNxIlIr"
      },
      "source": [
        "# Entrenamiento Final del Mejor Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7YhWm54Ptu7"
      },
      "outputs": [],
      "source": [
        "def train_best_bnn_model(best_model):\n",
        "    \"\"\"\n",
        "    Entrena el mejor modelo BNN encontrado con parámetros óptimos\n",
        "    \"\"\"\n",
        "    # Optimizador con schedule de tasa de aprendizaje (como en Binary.py)\n",
        "    initial_lr = 0.01\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "    best_model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Callbacks para entrenamiento avanzado\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            patience=15, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            factor=0.5, patience=10, verbose=1),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'best_bnn_model.h5', save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    # Normalización específica para BNN (-1 a 1 en lugar de 0 a 1)\n",
        "    # Las BNN funcionan mejor con datos centrados en 0\n",
        "    x_train_final_bnn = 2.0 * x_train_final - 1.0\n",
        "    x_val_bnn = 2.0 * x_val - 1.0\n",
        "    x_test_bnn = 2.0 * x_test - 1.0\n",
        "\n",
        "    # Entrenamiento\n",
        "    print(\"\\nEntrenando modelo BNN final...\")\n",
        "    history = best_model.fit(\n",
        "        x_train_final_bnn, y_train_final,\n",
        "        validation_data=(x_val_bnn, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Evaluación\n",
        "    test_loss, test_acc = best_model.evaluate(x_test_bnn, y_test)\n",
        "    print(f\"Precisión en conjunto de prueba: {test_acc:.4f}\")\n",
        "\n",
        "    # Visualizar resultados\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Precisión del modelo BNN')\n",
        "    plt.ylabel('Precisión')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend(['Entrenamiento', 'Validación'], loc='lower right')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Pérdida del modelo BNN')\n",
        "    plt.ylabel('Pérdida')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend(['Entrenamiento', 'Validación'], loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "    return best_model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnShfgIwIusJ"
      },
      "source": [
        "Función para Exportación y Preparación para STM32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQc2q3FyIyyI"
      },
      "outputs": [],
      "source": [
        "def export_bnn_for_stm32(model):\n",
        "    \"\"\"\n",
        "    Prepara el modelo BNN para su implementación en STM32\n",
        "    \"\"\"\n",
        "    # Guardar modelo\n",
        "    model.save('bnn_model_for_stm32.h5')\n",
        "\n",
        "    # Resumir parámetros binarios para implementación manual si es necesario\n",
        "    binary_weights = {}\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, binary_conv) or isinstance(layer, binary_dense):\n",
        "            weights = layer.get_weights()\n",
        "            # Los pesos ya están binarizados para inferencia\n",
        "            binary_weights[f\"layer_{i}\"] = weights\n",
        "\n",
        "    # Guardar pesos binarios como archivo numpy para referencia\n",
        "    np.save('bnn_binary_weights.npy', binary_weights)\n",
        "\n",
        "    print(\"Modelo guardado para STM32.\")\n",
        "    print(\"Para usar con STM32Cube.AI o implementación manual.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUI3FccI395"
      },
      "source": [
        "# Ejecución"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QDwkcXlI3jN",
        "outputId": "1a7e912e-7c50-431e-8882-30c4ef92ba5c"
      },
      "outputs": [],
      "source": [
        "# Verificar que los datos estén divididos correctamente\n",
        "if not 'x_val' in locals():\n",
        "    print(\"Dividiendo datos para validación...\")\n",
        "    x_train_final, x_val, y_train_final, y_val = train_test_split(\n",
        "        x_train, y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "# Ejecutar NAS para BNN\n",
        "best_bnn_model, best_hps = run_bnn_nas()\n",
        "\n",
        "# Entrenar mejor modelo\n",
        "final_model, history = train_best_bnn_model(best_bnn_model)\n",
        "\n",
        "# Exportar para STM32\n",
        "export_bnn_for_stm32(final_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CotL8pqmOptg"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
