{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de Dependencias e Importación de Librerías\n",
    "Instalación de PyWavelets, fCWT y otras dependencias necesarias. Importación de todas las bibliotecas requeridas para procesamiento de señales, visualización, construcción de espectrogramas de Mel y aprendizaje profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias necesarias\n",
    "!pip install PyWavelets\n",
    "!pip install librosa\n",
    "!git clone https://github.com/fastlib/fCWT.git\n",
    "!pip install fCWT\n",
    "!apt-get update\n",
    "!apt-get install libfftw3-single3 -y\n",
    "!pip install opencv-python\n",
    "\n",
    "# Importación de bibliotecas necesarias\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import random\n",
    "import h5py\n",
    "import cv2\n",
    "import pywt\n",
    "import fcwt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuraciones globales para visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y Visualización de Datos\n",
    "Carga de datos de acelerómetro desde Google Drive, con opciones para clasificación binaria o multiclase (cinco tipos). Visualización de las señales originales para explorar las diferencias entre distintos tipos de fugas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la ruta de los datos en Google Drive\n",
    "data_dir = '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/Branched'\n",
    "original_sr = 25600  # Frecuencia de muestreo original en Hz\n",
    "signal_sr = 25600  # Frecuencia de muestreo deseada en Hz\n",
    "downsample_factor = original_sr // signal_sr\n",
    "\n",
    "# Modo de clasificación: 'five_classes' o 'binary'\n",
    "classification_mode = 'five_classes'  # Cambiar según necesidad\n",
    "\n",
    "# Diccionario de etiquetas según el modo de clasificación\n",
    "if classification_mode == 'five_classes':\n",
    "    label_codes_dict = {\n",
    "        'Circumferential Crack': 0,\n",
    "        'Gasket Leak': 1,\n",
    "        'Longitudinal Crack': 2,\n",
    "        'No-leak': 3,\n",
    "        'Orifice Leak': 4\n",
    "    }\n",
    "else:  # binary\n",
    "    label_codes_dict = {'Leak': 0, 'No-leak': 1}\n",
    "\n",
    "# Función para eliminar archivos .DS_Store si existen\n",
    "def remove_DS_store_file(path):\n",
    "    for ds_name in ['.DS_Store', '.DS_store']:\n",
    "        ds_store_file_location = os.path.join(path, ds_name)\n",
    "        if os.path.isfile(ds_store_file_location):\n",
    "            os.remove(ds_store_file_location)\n",
    "\n",
    "# Función para cargar datos de acelerómetro\n",
    "def load_accelerometer_data(data_dir, sample_rate, downsample_factor, label_codes, mode='five_classes', fraction_to_include=1):\n",
    "    remove_DS_store_file(data_dir)\n",
    "    signals = []\n",
    "    labels = []\n",
    "\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if not os.path.isdir(label_dir):\n",
    "            print(f\"Omitiendo {label_dir} porque no es un directorio\")\n",
    "            continue\n",
    "\n",
    "        remove_DS_store_file(label_dir)\n",
    "\n",
    "        for file in os.listdir(label_dir):\n",
    "            file_path = os.path.join(label_dir, file)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "\n",
    "            accelerometer_signal_df = pd.read_csv(file_path, index_col=False)\n",
    "            accelerometer_signal_df = accelerometer_signal_df.iloc[::downsample_factor, :].reset_index(drop=True)\n",
    "            accelerometer_signal = accelerometer_signal_df['Value'][0:(sample_rate * 30)]\n",
    "\n",
    "            sample_indexes = np.linspace(0, len(accelerometer_signal) - sample_rate, len(accelerometer_signal) // sample_rate)\n",
    "            signal_frames_number = fraction_to_include * len(sample_indexes)\n",
    "            signal_frames_counter = 0\n",
    "\n",
    "            for signal_frame in sample_indexes:\n",
    "                accelerometer_signal_frame = accelerometer_signal[int(signal_frame):int(signal_frame + sample_rate)]\n",
    "                signal_frames_counter += 1\n",
    "\n",
    "                if signal_frames_counter > signal_frames_number:\n",
    "                    break\n",
    "\n",
    "                if len(accelerometer_signal_frame) != sample_rate:\n",
    "                    continue\n",
    "\n",
    "                if mode == 'five_classes':\n",
    "                    signals.append(accelerometer_signal_frame)\n",
    "                    labels.append(label_codes[label])\n",
    "                else:  # binary\n",
    "                    if label == 'No-leak':\n",
    "                        signals.append(accelerometer_signal_frame)\n",
    "                        labels.append(1)  # 1 para No-leak\n",
    "                    else:\n",
    "                        signals.append(accelerometer_signal_frame)\n",
    "                        labels.append(0)  # 0 para Leak\n",
    "\n",
    "    return signals, labels\n",
    "\n",
    "# Cargar los datos desde Google Drive\n",
    "signals_lst, labels_lst = load_accelerometer_data(\n",
    "    data_dir,\n",
    "    signal_sr,\n",
    "    downsample_factor,\n",
    "    label_codes_dict,\n",
    "    mode=classification_mode,\n",
    "    fraction_to_include=1\n",
    ")\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "signals_dict = {'training': [], 'testing': []}\n",
    "labels_dict = {'training': [], 'testing': []}\n",
    "\n",
    "signals_dict['training'], signals_dict['testing'], labels_dict['training'], labels_dict['testing'] = train_test_split(\n",
    "    signals_lst,\n",
    "    labels_lst,\n",
    "    test_size=0.2,\n",
    "    random_state=53\n",
    ")\n",
    "\n",
    "# Imprimir información sobre el dataset\n",
    "print(f'Data Directory: {data_dir}')\n",
    "print(f'Sample Rate: {signal_sr} Hz')\n",
    "print(f'Classification Mode: {classification_mode}')\n",
    "print(f'Number of signals (training, testing): ({len(signals_dict[\"training\"])}, {len(signals_dict[\"testing\"])})')\n",
    "print(f'Number of labels (training, testing): ({len(labels_dict[\"training\"])}, {len(labels_dict[\"testing\"])})')\n",
    "print(f'Number of samples per signal: {len(signals_dict[\"training\"][0])}')\n",
    "\n",
    "# Visualizar algunas señales\n",
    "plt.figure(figsize=(20, 20))\n",
    "rows, cols = 5, 2\n",
    "n = rows * cols\n",
    "random_index = []\n",
    "\n",
    "for i in range(n):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    random_index.append(np.random.randint(0, len(signals_dict['training'])))\n",
    "    plt.plot(signals_dict['training'][random_index[i]])\n",
    "\n",
    "    if classification_mode == 'five_classes':\n",
    "        label_name = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(labels_dict['training'][random_index[i]])]\n",
    "    else:\n",
    "        label_name = 'Leak' if labels_dict['training'][random_index[i]] == 0 else 'No-leak'\n",
    "\n",
    "    plt.title(label_name)\n",
    "    plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización y Denoising con Wavelet\n",
    "Aplicación de técnicas de denoising basadas en wavelet para eliminar el ruido en las señales. Normalización posterior para preparar los datos para la generación de espectrogramas Mel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para aplicar denoising con wavelet\n",
    "def wavelet_denoise(signals_dict, labels_dict):\n",
    "    \"\"\"\n",
    "    Normaliza y aplica denoising wavelet a las señales.\n",
    "\n",
    "    Args:\n",
    "        signals_dict: Diccionario con señales de entrenamiento y prueba.\n",
    "        labels_dict: Diccionario con etiquetas de entrenamiento y prueba.\n",
    "\n",
    "    Returns:\n",
    "        wavelet_denoised_signals: Diccionario con señales procesadas.\n",
    "        labels_dict: Diccionario con etiquetas.\n",
    "    \"\"\"\n",
    "    # Crear un objeto de la clase WaveletDenoising\n",
    "    wd = WaveletDenoising(\n",
    "        normalize=True,\n",
    "        wavelet='sym3',\n",
    "        level=4,\n",
    "        thr_mode='soft',\n",
    "        method=\"universal\"\n",
    "    )\n",
    "\n",
    "    # Crear un nuevo diccionario para almacenar las señales procesadas\n",
    "    wavelet_denoised_signals = {'training': [], 'testing': []}\n",
    "\n",
    "    for key, signals_subset in signals_dict.items():\n",
    "        for signal_element in tqdm(signals_subset, desc=f\"Denoising {key} signals\"):\n",
    "            # Aplicar denoising a la señal\n",
    "            denoised_signal = wd.fit(signal_element)\n",
    "            wavelet_denoised_signals[key].append(denoised_signal)\n",
    "\n",
    "    return wavelet_denoised_signals, labels_dict\n",
    "\n",
    "# Aplicar denoising a las señales\n",
    "wavelet_denoised_signals_dict, labels_dict = wavelet_denoise(signals_dict=signals_dict, labels_dict=labels_dict)\n",
    "\n",
    "# Imprimir información sobre el dataset resultante\n",
    "print(f'Number of signals (training, testing): ({len(wavelet_denoised_signals_dict[\"training\"])}, {len(wavelet_denoised_signals_dict[\"testing\"])})')\n",
    "print(f'Number of labels (training, testing): ({len(labels_dict[\"training\"])}, {len(labels_dict[\"testing\"])})')\n",
    "print(f'Number of samples per signal: {len(wavelet_denoised_signals_dict[\"training\"][0])}')\n",
    "\n",
    "# Visualizar algunas señales procesadas\n",
    "plt.figure(figsize=(20, 20))\n",
    "rows, cols = 5, 2\n",
    "n = rows * cols\n",
    "\n",
    "for i in range(n):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.plot(wavelet_denoised_signals_dict['training'][random_index[i]])\n",
    "\n",
    "    if classification_mode == 'five_classes':\n",
    "        label_name = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(labels_dict['training'][random_index[i]])]\n",
    "    else:\n",
    "        label_name = 'Leak' if labels_dict['training'][random_index[i]] == 0 else 'No-leak'\n",
    "\n",
    "    plt.title(label_name)\n",
    "    plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Función para normalizar señales\n",
    "def normalize_signals(signals_dict):\n",
    "    \"\"\"\n",
    "    Normaliza cada señal del diccionario a un rango de 0 a 1.\n",
    "\n",
    "    Args:\n",
    "        signals_dict: Diccionario con señales a normalizar.\n",
    "\n",
    "    Returns:\n",
    "        normalized_signals: Diccionario con señales normalizadas.\n",
    "    \"\"\"\n",
    "    normalized_signals = {'training': [], 'testing': []}\n",
    "\n",
    "    for key, signals_subset in signals_dict.items():\n",
    "        for signal in tqdm(signals_subset, desc=f\"Normalizando {key} signals\"):\n",
    "            min_val = np.min(signal)\n",
    "            max_val = np.max(signal)\n",
    "\n",
    "            if max_val > min_val:\n",
    "                normalized_signal = (signal - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                normalized_signal = np.ones_like(signal) * 0.5\n",
    "\n",
    "            normalized_signals[key].append(normalized_signal)\n",
    "\n",
    "    return normalized_signals\n",
    "\n",
    "# Normalizar las señales procesadas\n",
    "print(\"Normalizando señales procesadas con wavelet denoising...\")\n",
    "normalized_signals_dict = normalize_signals(wavelet_denoised_signals_dict)\n",
    "\n",
    "# Imprimir información sobre el dataset normalizado\n",
    "print(f'Number of signals (training, testing): ({len(normalized_signals_dict[\"training\"])}, {len(normalized_signals_dict[\"testing\"])})')\n",
    "print(f'Number of samples per signal: {len(normalized_signals_dict[\"training\"][0])}')\n",
    "\n",
    "# Comprobar rango de valores\n",
    "for key in normalized_signals_dict:\n",
    "    sample_signal = normalized_signals_dict[key][0]\n",
    "    print(f\"Rango de valores en {key}: [{np.min(sample_signal):.4f}, {np.max(sample_signal):.4f}]\")\n",
    "\n",
    "# Visualizar comparación de señales originales y normalizadas\n",
    "plt.figure(figsize=(20, 15))\n",
    "rows, cols = 3, 2\n",
    "sample_indices = random_index[:3]\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Señal con denoising (sin normalizar)\n",
    "    plt.subplot(rows, cols, i * 2 + 1)\n",
    "    plt.plot(wavelet_denoised_signals_dict['training'][idx])\n",
    "\n",
    "    if classification_mode == 'five_classes':\n",
    "        label_name = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(labels_dict['training'][idx])]\n",
    "    else:\n",
    "        label_name = 'Leak' if labels_dict['training'][idx] == 0 else 'No-leak'\n",
    "\n",
    "    plt.title(f\"Denoised: {label_name}\")\n",
    "    plt.grid()\n",
    "\n",
    "    # Señal normalizada entre 0 y 1\n",
    "    plt.subplot(rows, cols, i * 2 + 2)\n",
    "    plt.plot(normalized_signals_dict['training'][idx])\n",
    "    plt.title(f\"Normalized: {label_name}\")\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Usar las señales normalizadas para los pasos siguientes\n",
    "wavelet_denoised_signals_dict = normalized_signals_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación de Señales\n",
    "División de las señales normalizadas en segmentos de 512 muestras, similar al enfoque utilizado en la notebook de escalogramas CWT. Creación de etiquetas correspondientes para cada segmento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para segmentar señales en fragmentos de 512 muestras\n",
    "def segment_signals(signals_dict, labels_dict, segment_size=512):\n",
    "    \"\"\"\n",
    "    Segmenta las señales en fragmentos de tamaño fijo y crea etiquetas correspondientes.\n",
    "\n",
    "    Args:\n",
    "        signals_dict: Diccionario con señales de entrenamiento y prueba.\n",
    "        labels_dict: Diccionario con etiquetas de entrenamiento y prueba.\n",
    "        segment_size: Tamaño de cada segmento en muestras.\n",
    "\n",
    "    Returns:\n",
    "        segmented_signals_dict: Diccionario con segmentos de señales.\n",
    "        segmented_labels_dict: Diccionario con etiquetas correspondientes.\n",
    "    \"\"\"\n",
    "    segmented_signals_dict = {'training': [], 'testing': []}\n",
    "    segmented_labels_dict = {'training': [], 'testing': []}\n",
    "\n",
    "    for key in signals_dict.keys():\n",
    "        for signal, label in tqdm(zip(signals_dict[key], labels_dict[key]), desc=f\"Segmentando {key} signals\", total=len(signals_dict[key])):\n",
    "            num_segments = len(signal) // segment_size\n",
    "            for i in range(num_segments):\n",
    "                start_idx = i * segment_size\n",
    "                end_idx = start_idx + segment_size\n",
    "                segment = signal[start_idx:end_idx]\n",
    "                if len(segment) == segment_size:\n",
    "                    segmented_signals_dict[key].append(segment)\n",
    "                    segmented_labels_dict[key].append(label)\n",
    "\n",
    "    return segmented_signals_dict, segmented_labels_dict\n",
    "\n",
    "# Segmentar las señales normalizadas\n",
    "print(\"Segmentando señales normalizadas en fragmentos de 512 muestras...\")\n",
    "segmented_signals_dict, segmented_labels_dict = segment_signals(wavelet_denoised_signals_dict, labels_dict, segment_size=512)\n",
    "\n",
    "# Imprimir información sobre los segmentos generados\n",
    "print(f'Número de segmentos (training, testing): ({len(segmented_signals_dict[\"training\"])}, {len(segmented_signals_dict[\"testing\"])})')\n",
    "print(f'Número de etiquetas (training, testing): ({len(segmented_labels_dict[\"training\"])}, {len(segmented_labels_dict[\"testing\"])})')\n",
    "print(f'Tamaño de cada segmento: {len(segmented_signals_dict[\"training\"][0])} muestras')\n",
    "\n",
    "# Visualizar algunos segmentos\n",
    "plt.figure(figsize=(20, 20))\n",
    "rows, cols = 5, 2\n",
    "n = rows * cols\n",
    "random_segment_indices = random.sample(range(len(segmented_signals_dict['training'])), n)\n",
    "\n",
    "for i, idx in enumerate(random_segment_indices):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.plot(segmented_signals_dict['training'][idx])\n",
    "\n",
    "    if classification_mode == 'five_classes':\n",
    "        label_name = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(segmented_labels_dict['training'][idx])]\n",
    "    else:\n",
    "        label_name = 'Leak' if segmented_labels_dict['training'][idx] == 0 else 'No-leak'\n",
    "\n",
    "    plt.title(label_name)\n",
    "    plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Espectrogramas Mel por Segmentos\n",
    "Implementación del proceso de generación de espectrogramas logarítmicos de Mel para cada segmento de 512 muestras. Ajuste de los parámetros STFT y bancos de filtros para adaptarse a la menor duración de los segmentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar espectrogramas logarítmicos de Mel por segmentos\n",
    "def generate_mel_spectrograms_by_segments(segmented_signals_dict, segmented_labels_dict, fs=25600, n_mels=40, fmin=1, fmax=12800):\n",
    "    \"\"\"\n",
    "    Genera espectrogramas logarítmicos de Mel para cada segmento de señales.\n",
    "\n",
    "    Args:\n",
    "        segmented_signals_dict: Diccionario con segmentos de señales de entrenamiento y prueba.\n",
    "        segmented_labels_dict: Diccionario con etiquetas correspondientes.\n",
    "        fs: Frecuencia de muestreo en Hz.\n",
    "        n_mels: Número de bancos de filtros Mel.\n",
    "        fmin: Frecuencia mínima para los filtros Mel.\n",
    "        fmax: Frecuencia máxima para los filtros Mel.\n",
    "\n",
    "    Returns:\n",
    "        mel_spectrograms_dict: Diccionario con espectrogramas logarítmicos de Mel.\n",
    "        labels_dict: Diccionario con etiquetas correspondientes.\n",
    "    \"\"\"\n",
    "    # Parámetros para STFT\n",
    "    frame_length = int(0.02 * fs)  # 20 ms\n",
    "    frame_shift = int(0.01 * fs)  # 10 ms\n",
    "    nfft = 512  # Tamaño de FFT\n",
    "\n",
    "    # Crear diccionario para almacenar espectrogramas\n",
    "    mel_spectrograms_dict = {'training': [], 'testing': []}\n",
    "\n",
    "    for key, signals_subset in segmented_signals_dict.items():\n",
    "        for i, signal in enumerate(tqdm(signals_subset, desc=f\"Generando espectrogramas Mel para {key}\")):\n",
    "            # Calcular STFT\n",
    "            stft = librosa.stft(\n",
    "                signal,\n",
    "                n_fft=nfft,\n",
    "                hop_length=frame_shift,\n",
    "                win_length=frame_length,\n",
    "                window='hann'\n",
    "            )\n",
    "\n",
    "            # Calcular espectrograma de potencia\n",
    "            power_spectrogram = np.abs(stft) ** 2\n",
    "\n",
    "            # Aplicar bancos de filtros Mel\n",
    "            mel_filterbank = librosa.filters.mel(\n",
    "                sr=fs,\n",
    "                n_fft=nfft,\n",
    "                n_mels=n_mels,\n",
    "                fmin=fmin,\n",
    "                fmax=fmax\n",
    "            )\n",
    "            mel_spectrogram = np.dot(mel_filterbank, power_spectrogram)\n",
    "\n",
    "            # Convertir a escala logarítmica\n",
    "            log_mel_spectrogram = np.log1p(mel_spectrogram)\n",
    "\n",
    "            # Normalizar a rango [0, 1]\n",
    "            log_mel_spectrogram = (log_mel_spectrogram - np.min(log_mel_spectrogram)) / (np.max(log_mel_spectrogram) - np.min(log_mel_spectrogram))\n",
    "\n",
    "            # Almacenar en el diccionario\n",
    "            mel_spectrograms_dict[key].append(log_mel_spectrogram)\n",
    "\n",
    "    return mel_spectrograms_dict, segmented_labels_dict\n",
    "\n",
    "# Generar espectrogramas logarítmicos de Mel por segmentos\n",
    "mel_spectrograms_dict, mel_labels_dict = generate_mel_spectrograms_by_segments(\n",
    "    segmented_signals_dict,\n",
    "    segmented_labels_dict,\n",
    "    fs=signal_sr,\n",
    "    n_mels=40,\n",
    "    fmin=1,\n",
    "    fmax=12800\n",
    ")\n",
    "\n",
    "# Imprimir información sobre los espectrogramas generados\n",
    "print(f'Número de espectrogramas Mel (training, testing): ({len(mel_spectrograms_dict[\"training\"])}, {len(mel_spectrograms_dict[\"testing\"])})')\n",
    "print(f'Tamaño de un espectrograma Mel (ejemplo): {mel_spectrograms_dict[\"training\"][0].shape}')\n",
    "\n",
    "# Visualizar algunos espectrogramas logarítmicos de Mel\n",
    "plt.figure(figsize=(20, 20))\n",
    "rows, cols = 5, 2\n",
    "n = rows * cols\n",
    "random_mel_indices = random.sample(range(len(mel_spectrograms_dict['training'])), n)\n",
    "\n",
    "for i, idx in enumerate(random_mel_indices):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    librosa.display.specshow(\n",
    "        mel_spectrograms_dict['training'][idx],\n",
    "        sr=signal_sr,\n",
    "        hop_length=int(0.01 * signal_sr),\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        fmin=1,\n",
    "        fmax=12800,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    if classification_mode == 'five_classes':\n",
    "        label_name = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(mel_labels_dict['training'][idx])]\n",
    "    else:\n",
    "        label_name = 'Leak' if mel_labels_dict['training'][idx] == 0 else 'No-leak'\n",
    "    plt.title(label_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de Datos para el Modelo CNN\n",
    "Preparación de los espectrogramas logarítmicos de Mel para ser utilizados en el modelo CNN. Conversión a arrays NumPy, normalización, expansión de dimensiones y codificación one-hot de las etiquetas. Guardado del dataset en formato HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preparar los datos para el modelo CNN\n",
    "def prepare_data_for_model(mel_spectrograms_dict, mel_labels_dict, classification_mode):\n",
    "    \"\"\"\n",
    "    Prepara los datos para el modelo CNN.\n",
    "\n",
    "    Args:\n",
    "        mel_spectrograms_dict: Diccionario con espectrogramas logarítmicos de Mel.\n",
    "        mel_labels_dict: Diccionario con etiquetas correspondientes.\n",
    "        classification_mode: Modo de clasificación ('five_classes' o 'binary').\n",
    "\n",
    "    Returns:\n",
    "        x_train, y_train, x_test, y_test: Datos preparados para entrenamiento.\n",
    "        num_classes: Número de clases.\n",
    "    \"\"\"\n",
    "    # Convertir a arrays numpy\n",
    "    x_train = np.array(mel_spectrograms_dict['training'])\n",
    "    y_train = np.array(mel_labels_dict['training'])\n",
    "    x_test = np.array(mel_spectrograms_dict['testing'])\n",
    "    y_test = np.array(mel_labels_dict['testing'])\n",
    "\n",
    "    # Asegurar que los espectrogramas tengan valores normalizados\n",
    "    x_train = np.clip(x_train, 0, 1)\n",
    "    x_test = np.clip(x_test, 0, 1)\n",
    "\n",
    "    # Determinar número de clases según el modo\n",
    "    if classification_mode == 'five_classes':\n",
    "        num_classes = 5\n",
    "    else:  # binary\n",
    "        num_classes = 2\n",
    "\n",
    "    # One-hot encoding de las etiquetas\n",
    "    y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, y_train, y_train_onehot, x_test, y_test, y_test_onehot, num_classes\n",
    "\n",
    "# Preparar los datos para el modelo\n",
    "x_train, y_train, y_train_onehot, x_test, y_test, y_test_onehot, num_classes = prepare_data_for_model(\n",
    "    mel_spectrograms_dict,\n",
    "    mel_labels_dict,\n",
    "    classification_mode\n",
    ")\n",
    "\n",
    "# Imprimir información sobre los datos preparados\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train_onehot shape: {y_train_onehot.shape}')\n",
    "print(f'x_test shape: {x_test.shape}')\n",
    "print(f'y_test_onehot shape: {y_test_onehot.shape}')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "# Guardar en formato HDF5\n",
    "def save_dataset_to_h5(x_train, y_train, y_train_onehot, x_test, y_test, y_test_onehot,\n",
    "                       classification_mode, num_classes, label_codes_dict,\n",
    "                       file_path=None):\n",
    "    \"\"\"\n",
    "    Guarda el dataset en formato HDF5.\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        file_path = f'/content/drive/MyDrive/Tesis/Accelerometer_Dataset/mel_log_spectrogram_dataset_{classification_mode}.h5'\n",
    "\n",
    "    print(f\"Guardando dataset en {file_path}...\")\n",
    "    with h5py.File(file_path, 'w') as hf:\n",
    "        # Crear grupos para training y testing\n",
    "        train_group = hf.create_group('train')\n",
    "        test_group = hf.create_group('test')\n",
    "\n",
    "        # Guardar espectrogramas y etiquetas procesados\n",
    "        train_group.create_dataset('spectrograms', data=x_train)\n",
    "        train_group.create_dataset('labels', data=y_train)\n",
    "        train_group.create_dataset('labels_onehot', data=y_train_onehot)\n",
    "\n",
    "        test_group.create_dataset('spectrograms', data=x_test)\n",
    "        test_group.create_dataset('labels', data=y_test)\n",
    "        test_group.create_dataset('labels_onehot', data=y_test_onehot)\n",
    "\n",
    "        # Guardar metadatos\n",
    "        metadata = hf.create_group('metadata')\n",
    "        metadata.create_dataset('num_classes', data=num_classes)\n",
    "        metadata.create_dataset('shape', data=np.array(x_train.shape[1:]))\n",
    "        metadata.attrs['classification_mode'] = classification_mode\n",
    "\n",
    "        # Guardar diccionario de etiquetas\n",
    "        import json\n",
    "        if classification_mode == 'five_classes':\n",
    "            label_codes_json = json.dumps({k: int(v) for k, v in label_codes_dict.items()})\n",
    "        else:  # binary\n",
    "            label_codes_json = json.dumps({k: int(v) for k, v in {'Leak': 0, 'No-leak': 1}.items()})\n",
    "        metadata.attrs['label_codes_dict'] = label_codes_json\n",
    "\n",
    "    print(f\"Dataset procesado y guardado en {file_path}\")\n",
    "    print(f\"Datos de entrenamiento: {len(x_train)} muestras\")\n",
    "    print(f\"Datos de prueba: {len(x_test)} muestras\")\n",
    "\n",
    "    # Verificar la existencia del archivo guardado\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Archivo guardado correctamente. Tamaño: {os.path.getsize(file_path) / (1024*1024):.2f} MB\")\n",
    "    else:\n",
    "        print(\"Error: No se pudo guardar el archivo\")\n",
    "\n",
    "# Guardar el dataset\n",
    "save_dataset_to_h5(\n",
    "    x_train, y_train, y_train_onehot,\n",
    "    x_test, y_test, y_test_onehot,\n",
    "    classification_mode, num_classes, label_codes_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumento de Datos para Espectrogramas\n",
    "Implementación de técnicas de aumento de datos específicas para espectrogramas Mel, incluyendo desplazamiento de tiempo, enmascaramiento de frecuencia, adición de ruido gaussiano y estiramientos temporales para mejorar la generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para aplicar técnicas de aumento de datos a los espectrogramas Mel\n",
    "def apply_spectrogram_augmentation(x_train, y_train, augmentation_factor=2):\n",
    "    \"\"\"\n",
    "    Aplica técnicas de aumento de datos a los espectrogramas logarítmicos de Mel.\n",
    "\n",
    "    Args:\n",
    "        x_train: Espectrogramas de entrenamiento.\n",
    "        y_train: Etiquetas de entrenamiento.\n",
    "        augmentation_factor: Número de veces que se aumentará el dataset original.\n",
    "\n",
    "    Returns:\n",
    "        augmented_x_train: Espectrogramas aumentados.\n",
    "        augmented_y_train: Etiquetas correspondientes a los espectrogramas aumentados.\n",
    "    \"\"\"\n",
    "    augmented_x_train = []\n",
    "    augmented_y_train = []\n",
    "\n",
    "    for i in tqdm(range(len(x_train)), desc=\"Augmenting spectrograms\"):\n",
    "        spectrogram = x_train[i].copy()\n",
    "        label = y_train[i]\n",
    "\n",
    "        # Agregar el espectrograma original\n",
    "        augmented_x_train.append(spectrogram)\n",
    "        augmented_y_train.append(label)\n",
    "\n",
    "        for _ in range(augmentation_factor - 1):\n",
    "            # Crear una copia para manipular\n",
    "            augmented_spectrogram = spectrogram.copy()\n",
    "\n",
    "            # Desplazamiento de tiempo (adaptado para segmentos cortos)\n",
    "            time_shift = np.random.randint(-5, 5)  # Reducido para segmentos pequeños\n",
    "            augmented_spectrogram = np.roll(augmented_spectrogram, time_shift, axis=1)\n",
    "\n",
    "            # Enmascaramiento de frecuencia\n",
    "            freq_mask = np.random.randint(0, augmented_spectrogram.shape[0] // 4)\n",
    "            freq_start = np.random.randint(0, augmented_spectrogram.shape[0] - freq_mask)\n",
    "            augmented_spectrogram[freq_start:freq_start + freq_mask, :] = 0\n",
    "\n",
    "            # Ruido gaussiano\n",
    "            noise = np.random.normal(0, 0.01, augmented_spectrogram.shape)\n",
    "            augmented_spectrogram += noise\n",
    "\n",
    "            # Normalizar a rango [0, 1]\n",
    "            augmented_spectrogram = np.clip(augmented_spectrogram, 0, 1)\n",
    "\n",
    "            # Agregar el espectrograma aumentado\n",
    "            augmented_x_train.append(augmented_spectrogram)\n",
    "            augmented_y_train.append(label)\n",
    "\n",
    "    return np.array(augmented_x_train), np.array(augmented_y_train)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento para crear un conjunto de validación\n",
    "x_train_split, x_val, y_train_split, y_val = train_test_split(\n",
    "    x_train,  # Datos originales sin aumentar\n",
    "    y_train,  # Etiquetas originales sin aumentar\n",
    "    test_size=0.2,\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aplicar aumento de datos solo al conjunto de entrenamiento\n",
    "augmented_x_train, augmented_y_train = apply_spectrogram_augmentation(x_train_split, y_train_split)\n",
    "\n",
    "# Los datos finales para entrenar son los aumentados\n",
    "x_train_final = augmented_x_train\n",
    "y_train_final = augmented_y_train\n",
    "\n",
    "# Verificar tamaños\n",
    "print(f\"Datos originales de entrenamiento: {len(x_train_split)}\")\n",
    "print(f\"Datos aumentados de entrenamiento: {len(x_train_final)}\")\n",
    "print(f\"Datos de validación (sin aumentar): {len(x_val)}\")\n",
    "print(f\"Datos de prueba: {len(x_test)}\")\n",
    "\n",
    "# Visualizar algunos ejemplos de espectrogramas originales y aumentados\n",
    "plt.figure(figsize=(15, 10))\n",
    "rows, cols = 3, 2\n",
    "for i in range(rows):\n",
    "    # Índice aleatorio para seleccionar un ejemplo\n",
    "    idx = np.random.randint(0, len(x_train_split))\n",
    "    \n",
    "    # Espectrograma original\n",
    "    plt.subplot(rows, cols, i*2 + 1)\n",
    "    librosa.display.specshow(\n",
    "        x_train_split[idx],\n",
    "        sr=signal_sr,\n",
    "        hop_length=int(0.01 * signal_sr),\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        fmin=1,\n",
    "        fmax=12800,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f\"Original: {list(label_codes_dict.keys())[list(label_codes_dict.values()).index(y_train_split[idx])]}\")\n",
    "    \n",
    "    # Espectrograma aumentado\n",
    "    idx_aug = idx * augmentation_factor  # Índice en la versión aumentada\n",
    "    plt.subplot(rows, cols, i*2 + 2)\n",
    "    librosa.display.specshow(\n",
    "        x_train_final[idx_aug+1],  # +1 para tomar la primera versión aumentada\n",
    "        sr=signal_sr,\n",
    "        hop_length=int(0.01 * signal_sr),\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        fmin=1,\n",
    "        fmax=12800,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f\"Aumentado: {list(label_codes_dict.keys())[list(label_codes_dict.values()).index(y_train_final[idx_aug+1])]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación y Entrenamiento del Modelo ResNet18\n",
    "Construcción de un modelo ResNet18 adaptado para la clasificación de espectrogramas Mel de segmentos cortos. Implementación con regularización avanzada para prevenir el sobreajuste y callbacks especializados para monitorear y optimizar el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de bloques residuales para el modelo ResNet18\n",
    "def residual_block(x, filters, kernel_size=3, stride=1, conv_shortcut=False, l2_reg=1e-4):\n",
    "    \"\"\"\n",
    "    Bloque residual básico para ResNet con regularización L2.\n",
    "\n",
    "    Args:\n",
    "        x: Entrada al bloque residual.\n",
    "        filters: Número de filtros para las capas convolucionales.\n",
    "        kernel_size: Tamaño del kernel de las capas convolucionales.\n",
    "        stride: Stride para la primera capa convolucional.\n",
    "        conv_shortcut: Si True, aplica un atajo convolucional.\n",
    "        l2_reg: Regularización L2.\n",
    "\n",
    "    Returns:\n",
    "        Salida del bloque residual.\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    if conv_shortcut:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, padding='same',\n",
    "                               kernel_regularizer=regularizers.l2(l2_reg))(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same',\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same',\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.add([shortcut, x])\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_segment_mel_resnet18(input_shape, num_classes, l2_reg=2e-4, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Modelo ResNet18 adaptado para espectrogramas Mel de segmentos cortos.\n",
    "    Incluye regularización aumentada para prevenir sobreajuste.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Primera capa convolucional - adaptada para espectrogramas más pequeños\n",
    "    x = layers.Conv2D(64, 5, strides=1, padding='same',  # Kernel más pequeño y stride=1 para mantener resolución\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(2, strides=2, padding='same')(x)  # Reducción moderada de dimensionalidad\n",
    "\n",
    "    # Bloques residuales - adaptados para espectrogramas de segmentos\n",
    "    x = residual_block(x, 64, conv_shortcut=True, l2_reg=l2_reg)\n",
    "    x = residual_block(x, 64, l2_reg=l2_reg)\n",
    "\n",
    "    x = residual_block(x, 128, stride=2, conv_shortcut=True, l2_reg=l2_reg)\n",
    "    x = residual_block(x, 128, l2_reg=l2_reg)\n",
    "\n",
    "    x = residual_block(x, 256, stride=2, conv_shortcut=True, l2_reg=l2_reg)\n",
    "    x = residual_block(x, 256, l2_reg=l2_reg)\n",
    "\n",
    "    x = residual_block(x, 512, stride=2, conv_shortcut=True, l2_reg=l2_reg)\n",
    "    x = residual_block(x, 512, l2_reg=l2_reg)\n",
    "\n",
    "    # Capas finales con mayor regularización\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)  # Dropout alto para prevenir sobreajuste\n",
    "    outputs = layers.Dense(num_classes, activation='softmax',\n",
    "                         kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"segment_mel_resnet18\")\n",
    "    return model\n",
    "\n",
    "# Preparar los datos para el entrenamiento\n",
    "# Asegurarse de que y_train_final y y_val estén en formato one-hot\n",
    "y_train_final_onehot = keras.utils.to_categorical(y_train_final, num_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# Añadir dimensión de canal si no existe\n",
    "if len(x_train_final.shape) == 3:\n",
    "    x_train_final = x_train_final[..., np.newaxis]\n",
    "if len(x_val.shape) == 3:\n",
    "    x_val = x_val[..., np.newaxis]\n",
    "if len(x_test.shape) == 3:\n",
    "    x_test = x_test[..., np.newaxis]\n",
    "\n",
    "print(f\"Forma de datos de entrenamiento: {x_train_final.shape}\")\n",
    "print(f\"Forma de datos de validación: {x_val.shape}\")\n",
    "print(f\"Forma de datos de prueba: {x_test.shape}\")\n",
    "\n",
    "# Construir el modelo\n",
    "input_shape = x_train_final.shape[1:]  # Forma de entrada\n",
    "model = build_segment_mel_resnet18(\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes,\n",
    "    l2_reg=2e-4,  # Regularización L2 moderada\n",
    "    dropout_rate=0.5  # Dropout alto\n",
    ")\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Función para warmup de la tasa de aprendizaje\n",
    "def warmup_schedule(epoch, lr):\n",
    "    warmup_epochs = 5\n",
    "    init_lr = 1e-6\n",
    "    target_lr = 0.0005\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        return init_lr + (target_lr - init_lr) * epoch / warmup_epochs\n",
    "    return lr\n",
    "\n",
    "# Callbacks para monitoreo y optimización\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/content/drive/MyDrive/Tesis/Models/segment_mel_spect_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    keras.callbacks.LearningRateScheduler(warmup_schedule)\n",
    "]\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    x_train_final, y_train_final_onehot,\n",
    "    validation_data=(x_val, y_val_onehot),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('/content/drive/MyDrive/Tesis/Models/segment_mel_spect_model_final.h5')\n",
    "\n",
    "# Visualizar la evolución del entrenamiento\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Gráfico de pérdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Validación')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de precisión\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Validación')\n",
    "plt.title('Precisión durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del Rendimiento del Modelo\n",
    "Evaluación exhaustiva del modelo entrenado utilizando métricas como precisión, recall, F1-score y matriz de confusión. Análisis detallado del rendimiento por clase y visualización de predicciones en ejemplos específicos para comprender mejor el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo en el conjunto de prueba\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test_onehot, verbose=1)\n",
    "print(f\"Pérdida en test: {test_loss:.4f}\")\n",
    "print(f\"Precisión en test: {test_accuracy:.4f}\")\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = y_test if len(y_test.shape) == 1 else np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualizar matriz de confusión\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    conf_matrix, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=list(label_codes_dict.keys()),\n",
    "    yticklabels=list(label_codes_dict.keys())\n",
    ")\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.title('Matriz de confusión')\n",
    "plt.show()\n",
    "\n",
    "# Generar informe de clasificación\n",
    "class_report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=list(label_codes_dict.keys()),\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convertir el informe a un DataFrame de pandas para mejor visualización\n",
    "class_report_df = pd.DataFrame(class_report).transpose()\n",
    "print(\"Informe de clasificación:\")\n",
    "display(class_report_df)\n",
    "\n",
    "# Visualizar ejemplos de predicciones correctas e incorrectas\n",
    "def plot_predictions(x_data, y_true, y_pred, num_examples=5):\n",
    "    # Encontrar ejemplos correctos e incorrectos\n",
    "    correct_indices = np.where(y_true == y_pred)[0]\n",
    "    incorrect_indices = np.where(y_true != y_pred)[0]\n",
    "    \n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Visualizar predicciones correctas\n",
    "    for i in range(min(num_examples, len(correct_indices))):\n",
    "        idx = correct_indices[i]\n",
    "        plt.subplot(2, num_examples, i + 1)\n",
    "        \n",
    "        # Eliminar la dimensión del canal si es necesario\n",
    "        spec_to_plot = x_data[idx].squeeze() if x_data[idx].ndim > 2 else x_data[idx]\n",
    "        \n",
    "        librosa.display.specshow(\n",
    "            spec_to_plot,\n",
    "            sr=signal_sr,\n",
    "            hop_length=int(0.01 * signal_sr),\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            fmin=1,\n",
    "            fmax=12800,\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        \n",
    "        true_label = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(y_true[idx])]\n",
    "        plt.title(f\"Correcto: {true_label}\")\n",
    "    \n",
    "    # Visualizar predicciones incorrectas\n",
    "    for i in range(min(num_examples, len(incorrect_indices))):\n",
    "        idx = incorrect_indices[i]\n",
    "        plt.subplot(2, num_examples, num_examples + i + 1)\n",
    "        \n",
    "        # Eliminar la dimensión del canal si es necesario\n",
    "        spec_to_plot = x_data[idx].squeeze() if x_data[idx].ndim > 2 else x_data[idx]\n",
    "        \n",
    "        librosa.display.specshow(\n",
    "            spec_to_plot,\n",
    "            sr=signal_sr,\n",
    "            hop_length=int(0.01 * signal_sr),\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            fmin=1,\n",
    "            fmax=12800,\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        \n",
    "        true_label = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(y_true[idx])]\n",
    "        pred_label = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(y_pred[idx])]\n",
    "        plt.title(f\"Incorrecto: Real {true_label}, Pred {pred_label}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar ejemplos de predicciones\n",
    "plot_predictions(x_test, y_true, y_pred, num_examples=5)\n",
    "\n",
    "# Análisis de rendimiento por clase\n",
    "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(label_codes_dict.keys()), y=class_accuracy)\n",
    "plt.title('Precisión por clase')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Precisión')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analizar los casos más difíciles (predicciones con menor confianza)\n",
    "prediction_confidence = np.max(y_pred_probs, axis=1)\n",
    "lower_confidence_indices = np.argsort(prediction_confidence)[:10]  # 10 predicciones con menor confianza\n",
    "\n",
    "print(\"Casos con menor confianza en la predicción:\")\n",
    "for idx in lower_confidence_indices:\n",
    "    true_label = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(y_true[idx])]\n",
    "    pred_label = list(label_codes_dict.keys())[list(label_codes_dict.values()).index(y_pred[idx])]\n",
    "    confidence = prediction_confidence[idx]\n",
    "    print(f\"Índice: {idx}, Real: {true_label}, Predicción: {pred_label}, Confianza: {confidence:.4f}\")\n",
    "\n",
    "# Guardar resultados de evaluación\n",
    "evaluation_results = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'confusion_matrix': conf_matrix.tolist(),\n",
    "    'classification_report': class_report\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('/content/drive/MyDrive/Tesis/Models/segment_mel_spect_evaluation.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f)\n",
    "\n",
    "print(\"Evaluación completa. Resultados guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
