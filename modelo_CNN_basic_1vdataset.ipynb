{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando los modulos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "!pip install qkeras keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import h5py\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    tf.data.experimental.enable_debug_mode()\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    from tensorflow.keras.layers import Input, MaxPooling2D, Flatten, Dropout, Add\n",
    "    from tensorflow.keras.models import Model\n",
    "    import matplotlib.pyplot as plt\n",
    "    from qkeras import *\n",
    "    import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cargando el dataset de escalogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "file_path = '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/accelerometer_BR_256NS_20Scales_cwt_dataset.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cwt_dataset(file_path='accelerometer_cwt_dataset.h5'):\n",
    "    \"\"\"\n",
    "    Carga el dataset de escalogramas preprocesados desde un archivo HDF5.\n",
    "\n",
    "    Args:\n",
    "        file_path: Ruta al archivo HDF5\n",
    "\n",
    "    Returns:\n",
    "        dataset: Un diccionario con los datos listos para entrenar\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        # Cargar escalogramas y etiquetas (ya normalizados, con canal y one-hot)\n",
    "        x_train = np.array(hf['train']['scalograms'])\n",
    "        y_train = np.array(hf['train']['labels_onehot'])  # Usar etiquetas one-hot\n",
    "        y_train_raw = np.array(hf['train']['labels'])     # También cargar etiquetas sin one-hot\n",
    "\n",
    "        x_test = np.array(hf['test']['scalograms'])\n",
    "        y_test = np.array(hf['test']['labels_onehot'])    # Usar etiquetas one-hot\n",
    "        y_test_raw = np.array(hf['test']['labels'])       # También cargar etiquetas sin one-hot\n",
    "\n",
    "        # Cargar metadatos\n",
    "        num_classes = hf['metadata']['num_classes'][()]\n",
    "        shape = tuple(hf['metadata']['shape'][()])\n",
    "        max_val = hf['metadata']['max_val'][()]  # Valor máximo usado en normalización\n",
    "\n",
    "        # Cargar diccionario de etiquetas\n",
    "        label_codes_dict = json.loads(hf['metadata'].attrs['label_codes_dict'])\n",
    "\n",
    "    # Organizar datos como un dataset tipo Keras (ya preprocesados)\n",
    "    dataset = {\n",
    "        'train': (x_train, y_train, y_train_raw),\n",
    "        'test': (x_test, y_test, y_test_raw),\n",
    "        'num_classes': num_classes,\n",
    "        'input_shape': shape,\n",
    "        'label_codes_dict': label_codes_dict,\n",
    "        'max_val': max_val\n",
    "    }\n",
    "\n",
    "    print(f\"Dataset cargado: {x_train.shape[0]} muestras de entrenamiento, {x_test.shape[0]} muestras de prueba\")\n",
    "    print(f\"Forma de cada escalograma: {shape}\")\n",
    "    print(f\"Número de clases: {num_classes}\")\n",
    "    print(f\"Rango de valores: [0, {max_val}] (ya normalizados)\")\n",
    "\n",
    "    # Verificar que ya están normalizados\n",
    "    print(f\"Valor máximo en datos de entrenamiento: {np.max(x_train)}\")\n",
    "    print(f\"Valor mínimo en datos de entrenamiento: {np.min(x_train)}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el dataset (ya preprocesado)\n",
    "file_path = '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/accelerometer_cwt_dataset_ns512_processed.h5'\n",
    "dataset = load_cwt_dataset(file_path)\n",
    "\n",
    "# Obtén las partes del dataset (ya normalizadas y con one-hot)\n",
    "x_train, y_train, y_train_raw = dataset['train']\n",
    "x_test, y_test, y_test_raw = dataset['test']\n",
    "input_shape = dataset['input_shape']\n",
    "num_classes = dataset['num_classes']\n",
    "\n",
    "print(\"Datos listos para entrenamiento:\")\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Verificando la normalización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si los datos ya están normalizados\n",
    "print(\"Estado actual de los datos:\")\n",
    "print(\"Valor máximo en x_train:\", np.max(x_train))\n",
    "print(\"Valor mínimo en x_train:\", np.min(x_train))\n",
    "print(\"Media de x_train:\", np.mean(x_train))\n",
    "print(\"Desviación estándar de x_train:\", np.std(x_train))\n",
    "\n",
    "# Visualizar la distribución de valores\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(x_train.flatten(), bins=50)\n",
    "plt.title('Distribución de valores en x_train')\n",
    "plt.xlabel('Valor')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Separamos una parte de los datos para la validacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos\n",
    "x_train_final, x_val, y_train_final, y_val = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size=0.2,  # 20% para validación\n",
    "    random_state=42,  # Para reproducibilidad\n",
    "    stratify=y_train  # Mantener la distribución de clases\n",
    ")\n",
    "\n",
    "# Verificar tamaños\n",
    "print(f\"Datos de entrenamiento: {x_train_final.shape[0]} muestras\")\n",
    "print(f\"Datos de validación: {x_val.shape[0]} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo CNN básico (Tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Crear un modelo CNN pequeño con técnicas modernas\n",
    "def create_tiny_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Crea un modelo CNN compacto optimizado para microcontroladores.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Capa de entrada\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Bloque 1: 8 filtros, escalado gradual\n",
    "        layers.Conv2D(8, kernel_size=(3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Bloque 2: 16 filtros con conexión residual\n",
    "        layers.Conv2D(16, kernel_size=(3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Bloque 3: 32 filtros (más capas en etapas posteriores)\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Capa de salida\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),  # Dropout para reducir overfitting\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "# Crear el modelo\n",
    "input_shape = x_train_final.shape[1:]\n",
    "num_classes = 5\n",
    "tiny_cnn = create_tiny_cnn_model(input_shape, num_classes)\n",
    "tiny_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar modelo\n",
    "tiny_cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks para mejorar el entrenamiento\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    ModelCheckpoint('best_tiny_cnn_model.h5', save_best_only=True, monitor='val_accuracy')\n",
    "]\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = tiny_cnn.fit(\n",
    "    x_train_final, y_train_final,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el proceso de entrenamiento\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_acc = tiny_cnn.evaluate(x_test, y_test)\n",
    "print(f\"Precisión en el conjunto de prueba: {test_acc:.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "# Matriz de confusión\n",
    "y_pred = np.argmax(tiny_cnn.predict(x_test), axis=1)\n",
    "y_true = y_test_raw  # Usar las etiquetas originales sin one-hot\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.show()\n",
    "\n",
    "# Para mostrar nombres en lugar de códigos en reportes\n",
    "class_names = list(dataset['label_codes_dict'].keys())\n",
    "print(classification_report(y_test_raw, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Avanzado con Conexiones Residuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiny_resnet_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Crea un modelo CNN con conexiones residuales.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Bloque 1\n",
    "    x = layers.Conv2D(8, kernel_size=(3, 3), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Bloque 2 con conexión residual\n",
    "    residual = x\n",
    "    x = layers.Conv2D(16, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Proyección de la conexión residual\n",
    "    residual = layers.Conv2D(16, kernel_size=(1, 1), padding='same')(residual)\n",
    "    x = layers.add([x, residual])  # Conexión residual\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Bloque 3 con conexión residual\n",
    "    residual = x\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Proyección de la conexión residual\n",
    "    residual = layers.Conv2D(32, kernel_size=(1, 1), padding='same')(residual)\n",
    "    x = layers.add([x, residual])  # Conexión residual\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Capa de salida\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Crear el modelo mejorado\n",
    "resnet_tiny = create_tiny_resnet_model(input_shape, num_classes)\n",
    "resnet_tiny.summary()\n",
    "\n",
    "# Compilar el modelo\n",
    "resnet_tiny.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss= 'categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history_resnet = resnet_tiny.fit(\n",
    "    x_train_final, y_train_final,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluar en el conjunto de prueba\n",
    "test_loss, test_acc = resnet_tiny.evaluate(x_test, y_test)\n",
    "print(f\"Precisión en el conjunto de prueba (Modelo ResNet): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportación del modelo para STM32CubeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a TFLite (formato intermedio para STM32CubeAI)\n",
    "def convert_to_tflite(model, model_name=\"tiny_cnn_model\"):\n",
    "    # Convertir el modelo a TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Guardar el modelo TFLite\n",
    "    tflite_model_path = f\"{model_name}.tflite\"\n",
    "    with open(tflite_model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"Modelo guardado como {tflite_model_path}\")\n",
    "    \n",
    "    # También guardar el modelo original en formato .h5\n",
    "    model.save(f\"{model_name}.h5\")\n",
    "    print(f\"Modelo original guardado como {model_name}.h5\")\n",
    "    \n",
    "    return tflite_model_path\n",
    "\n",
    "# Convertir ambos modelos\n",
    "tflite_basic = convert_to_tflite(tiny_cnn, \"tiny_cnn_model\")\n",
    "tflite_resnet = convert_to_tflite(resnet_tiny, \"tiny_resnet_model\")\n",
    "\n",
    "print(\"\\nPara utilizar estos modelos con STM32CubeAI:\")\n",
    "print(\"1. Abra STM32CubeAI\")\n",
    "print(\"2. Importe el archivo .h5 o .tflite\")\n",
    "print(\"3. Configure la validación y generación de código C\")\n",
    "print(\"4. Integre el código generado en su proyecto STM32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Architecture Search (NAS) multiobjetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Función de evaluación multiobjetivo personalizada\n",
    "def multi_obj_score(val_accuracy, model_params):\n",
    "    # Balancear precisión vs tamaño del modelo\n",
    "    # Alpha controla la importancia relativa de precisión vs tamaño\n",
    "    alpha = 0.7  # 70% importancia en precisión, 30% en tamaño\n",
    "    \n",
    "    # Normalizar número de parámetros (asumiendo un máximo de 1M params)\n",
    "    normalized_params = model_params / 1_000_000\n",
    "    \n",
    "    # Penalizar modelos muy grandes\n",
    "    size_score = 1.0 - normalized_params\n",
    "    \n",
    "    # Puntuación total (combinación ponderada)\n",
    "    total_score = alpha * val_accuracy + (1 - alpha) * size_score\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "# Función para construir el modelo con búsqueda de hiperparámetros\n",
    "def model_builder(hp):\n",
    "    # Hiperparámetros para buscar\n",
    "    filters_1 = hp.Int('filters_1', min_value=8, max_value=32, step=8)\n",
    "    filters_2 = hp.Int('filters_2', min_value=16, max_value=64, step=16)\n",
    "    filters_3 = hp.Int('filters_3', min_value=32, max_value=128, step=32)\n",
    "    \n",
    "    kernel_size = hp.Choice('kernel_size', values=[3, 5])\n",
    "    \n",
    "    use_batch_norm = hp.Boolean('batch_norm')\n",
    "    use_dropout = hp.Boolean('dropout')\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    use_residual = hp.Boolean('residual')\n",
    "    \n",
    "    # Construir modelo\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Primera etapa\n",
    "    x = layers.Conv2D(filters_1, kernel_size=kernel_size, padding='same')(inputs)\n",
    "    if use_batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    # Segunda etapa (posiblemente con conexión residual)\n",
    "    prev_x = x\n",
    "    \n",
    "    x = layers.Conv2D(filters_2, kernel_size=kernel_size, padding='same')(x)\n",
    "    if use_batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    if use_residual:\n",
    "        # Proyectar residual si es necesario\n",
    "        if filters_1 != filters_2:\n",
    "            prev_x = layers.Conv2D(filters_2, kernel_size=1, padding='same')(prev_x)\n",
    "        x = layers.add([x, prev_x])\n",
    "    \n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    # Tercera etapa\n",
    "    prev_x = x\n",
    "    \n",
    "    x = layers.Conv2D(filters_3, kernel_size=kernel_size, padding='same')(x)\n",
    "    if use_batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    if use_residual:\n",
    "        # Proyectar residual si es necesario\n",
    "        if filters_2 != filters_3:\n",
    "            prev_x = layers.Conv2D(filters_3, kernel_size=1, padding='same')(prev_x)\n",
    "        x = layers.add([x, prev_x])\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        ),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Clase personalizada para optimización multiobjetivo\n",
    "class MultiObjectiveHyperModel(kt.HyperModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def build(self, hp):\n",
    "        return model_builder(hp)\n",
    "        \n",
    "    def fit(self, hp, model, x_train, y_train, **kwargs):\n",
    "        return model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=hp.Choice('batch_size', [16, 32, 64]),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# Clase personalizada para Tuner con evaluación multiobjetivo\n",
    "class MultiObjectiveTuner(kt.Tuner):\n",
    "    def on_trial_end(self, trial):\n",
    "        # Obtener precisión de validación\n",
    "        val_accuracy = trial.metrics.get_last_value('val_accuracy') or 0\n",
    "        \n",
    "        # Obtener número de parámetros del modelo\n",
    "        model_params = trial.trial.executions[0].model.count_params()\n",
    "        \n",
    "        # Calcular puntuación multiobjetivo\n",
    "        score = multi_obj_score(val_accuracy, model_params)\n",
    "        \n",
    "        # Registrar métricas adicionales\n",
    "        trial.metrics.update({'model_size': model_params, 'multi_obj_score': score})\n",
    "        \n",
    "        # Ordenar por la puntuación multiobjetivo en lugar de solo precisión\n",
    "        self.oracle.objective.name = 'multi_obj_score'\n",
    "        trial.score = score\n",
    "        \n",
    "        super().on_trial_end(trial)\n",
    "\n",
    "# Inicializar el tuner\n",
    "tuner = MultiObjectiveTuner(\n",
    "    hypermodel=MultiObjectiveHyperModel(),\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective=kt.Objective('multi_obj_score', direction='max'),\n",
    "        max_trials=20\n",
    "    ),\n",
    "    directory='nas_search',\n",
    "    project_name='multi_obj_cnn'\n",
    ")\n",
    "\n",
    "# Buscar la mejor arquitectura\n",
    "tuner.search(\n",
    "    x_train_final, y_train_final,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=15,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Obtener los mejores modelos y mostrar resultados\n",
    "best_models = tuner.get_best_models(num_models=3)\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=3)\n",
    "\n",
    "for i, (model, hp) in enumerate(zip(best_models, best_hps)):\n",
    "    print(f\"\\nModelo #{i+1}:\")\n",
    "    print(f\"Hiperparámetros: {hp.values}\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Evaluar en el conjunto de prueba\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f\"Precisión en prueba: {test_acc:.4f}\")\n",
    "    print(f\"Número de parámetros: {model.count_params():,}\")\n",
    "    \n",
    "    # Guardar el mejor modelo para STM32CubeAI\n",
    "    if i == 0:\n",
    "        convert_to_tflite(model, f\"nas_optimized_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del Modelo Optimizado y Preparación para STM32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el mejor modelo de la búsqueda\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Visualizar la arquitectura\n",
    "tf.keras.utils.plot_model(best_model, show_shapes=True, dpi=70)\n",
    "\n",
    "# Análisis del tamaño del modelo\n",
    "print(\"\\nAnálisis del modelo optimizado:\")\n",
    "print(f\"Número total de parámetros: {best_model.count_params():,}\")\n",
    "\n",
    "# Tamaño del modelo en memoria\n",
    "def get_model_memory_usage(model):\n",
    "    shapes = [tf.TensorShape(layer.output_shape) for layer in model.layers]\n",
    "    dtypes = [layer.dtype for layer in model.layers]\n",
    "    \n",
    "    memory = 0\n",
    "    for shape, dtype in zip(shapes, dtypes):\n",
    "        if shape.is_fully_defined():\n",
    "            memory += np.prod(shape.as_list()) * np.dtype(dtype.name).itemsize\n",
    "    \n",
    "    return memory / (1024 * 1024)  # MB\n",
    "\n",
    "print(f\"Memoria estimada: {get_model_memory_usage(best_model):.2f} MB\")\n",
    "\n",
    "# Cuantización para reducir aún más el tamaño\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Datos representativos para la cuantización\n",
    "def representative_dataset():\n",
    "    for i in range(min(100, len(X_train))):\n",
    "        yield [np.expand_dims(X_train[i], axis=0).astype(np.float32)]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# Convertir y guardar modelo cuantizado\n",
    "tflite_quant_model = converter.convert()\n",
    "with open('tiny_cnn_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"\\nModelo cuantizado guardado como 'tiny_cnn_quantized.tflite'\")\n",
    "print(\"Este modelo está optimizado para su despliegue en STM32 a través de STM32CubeAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
