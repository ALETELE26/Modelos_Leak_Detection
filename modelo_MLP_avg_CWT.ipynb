{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Título: \"Modelo MLP para clasificación usando características CWT promediadas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuración inicial y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from keras import backend as K\n",
    "import keras_tuner as kt\n",
    "from google.colab import drive\n",
    "\n",
    "# Montar Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Función para cargar el dataset de características promediadas\n",
    "def load_cwt_averaged_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Carga el dataset de características CWT promediadas desde archivo H5\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        # Cargar características y etiquetas\n",
    "        x_train = np.array(hf['train']['features'])\n",
    "        y_train = np.array(hf['train']['labels_onehot'])\n",
    "        y_train_raw = np.array(hf['train']['labels'])\n",
    "        \n",
    "        x_test = np.array(hf['test']['features'])\n",
    "        y_test = np.array(hf['test']['labels_onehot'])\n",
    "        y_test_raw = np.array(hf['test']['labels'])\n",
    "        \n",
    "        # Cargar metadatos\n",
    "        num_classes = hf['metadata']['num_classes'][()]\n",
    "        num_features = hf['metadata']['num_features'][()]\n",
    "        \n",
    "        # Cargar diccionario de etiquetas\n",
    "        label_codes_dict = json.loads(hf['metadata'].attrs['label_codes_dict'])\n",
    "    \n",
    "    print(f\"Dataset cargado exitosamente:\")\n",
    "    print(f\"- Características de entrenamiento: {x_train.shape}\")\n",
    "    print(f\"- Características de prueba: {x_test.shape}\")\n",
    "    print(f\"- Número de clases: {num_classes}\")\n",
    "    print(f\"- Dimensiones por muestra: {num_features}\")\n",
    "    \n",
    "    # Visualizar distribución de clases\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    class_counts = np.sum(y_train, axis=0)\n",
    "    sns.barplot(x=list(label_codes_dict.keys()), y=class_counts)\n",
    "    plt.title(\"Distribución de clases en conjunto de entrenamiento\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'x_train': x_train,\n",
    "        'y_train': y_train,\n",
    "        'y_train_raw': y_train_raw,\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test,\n",
    "        'y_test_raw': y_test_raw,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features,\n",
    "        'label_codes_dict': label_codes_dict\n",
    "    }\n",
    "\n",
    "# Cargar dataset\n",
    "dataset_path = '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/accelerometer_cwt_averaged_ns512_processed.h5'\n",
    "dataset = load_cwt_averaged_dataset(dataset_path)\n",
    "\n",
    "# Dividir datos para tener un conjunto de validación separado\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    dataset['x_train'], dataset['y_train'], \n",
    "    test_size=0.2, stratify=dataset['y_train_raw'], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de entrenamiento: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Conjunto de validación: {x_val.shape}, {y_val.shape}\")\n",
    "print(f\"Conjunto de prueba: {dataset['x_test'].shape}, {dataset['y_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modelo MLP básico de referencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear un modelo MLP básico como referencia\n",
    "def create_base_mlp(input_dim, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y entrenar modelo base\n",
    "base_model = create_base_mlp(dataset['num_features'], dataset['num_classes'])\n",
    "base_model.summary()\n",
    "\n",
    "# Entrenar modelo base\n",
    "base_history = base_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluar modelo base\n",
    "base_test_loss, base_test_acc = base_model.evaluate(dataset['x_test'], dataset['y_test'])\n",
    "print(f\"Precisión del modelo base en datos de prueba: {base_test_acc:.4f}\")\n",
    "\n",
    "# Visualizar curvas de aprendizaje\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(base_history.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(base_history.history['val_accuracy'], label='Validación')\n",
    "plt.title('Precisión del modelo base')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(base_history.history['loss'], label='Entrenamiento')\n",
    "plt.plot(base_history.history['val_loss'], label='Validación')\n",
    "plt.title('Pérdida del modelo base')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modelo MLP con regularización avanzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regularized_mlp(input_dim, num_classes):\n",
    "    \"\"\"\n",
    "    Crea un modelo MLP con técnicas de regularización del estado del arte\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Primera capa con regularización L2\n",
    "        layers.Dense(128, \n",
    "                    kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Segunda capa con regularización L2\n",
    "        layers.Dense(64, \n",
    "                    kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Tercera capa\n",
    "        layers.Dense(32),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Capa de salida\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Learning rate con decay\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.001,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    \n",
    "    # Gradient clipping\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=lr_schedule,\n",
    "        clipnorm=1.0  # Gradient clipping\n",
    "    )\n",
    "    \n",
    "    # Label smoothing\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y entrenar modelo regularizado\n",
    "reg_model = create_regularized_mlp(dataset['num_features'], dataset['num_classes'])\n",
    "reg_model.summary()\n",
    "\n",
    "# Callbacks para entrenamiento\n",
    "callbacks = [\n",
    "    # Early stopping\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=30, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reducción de learning rate cuando se estanca\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Checkpoint para guardar el mejor modelo\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/best_reg_mlp.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar modelo regularizado\n",
    "reg_history = reg_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluar modelo regularizado\n",
    "reg_test_loss, reg_test_acc = reg_model.evaluate(dataset['x_test'], dataset['y_test'])\n",
    "print(f\"Precisión del modelo regularizado en datos de prueba: {reg_test_acc:.4f}\")\n",
    "\n",
    "# Comparar con modelo base\n",
    "print(f\"Mejora sobre el modelo base: {(reg_test_acc - base_test_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NAS multiobjetivo con Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir clase para búsqueda NAS multiobjetivo\n",
    "class MLPHyperModel(kt.HyperModel):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def build(self, hp):\n",
    "        \"\"\"\n",
    "        Construye un modelo MLP con hiperparámetros ajustables\n",
    "        \"\"\"\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(self.input_dim,)))\n",
    "        \n",
    "        # Determinar número de capas (entre 2 y 4)\n",
    "        n_layers = hp.Int('num_layers', 2, 4)\n",
    "        \n",
    "        # Configurar capas ocultas\n",
    "        for i in range(n_layers):\n",
    "            # Número de neuronas por capa\n",
    "            units = hp.Int(f'units_{i}', \n",
    "                          min_value=16, \n",
    "                          max_value=128, \n",
    "                          step=16)\n",
    "            \n",
    "            # Regularización L2\n",
    "            reg_rate = hp.Choice(f'reg_{i}', \n",
    "                                values=[0.0, 0.0001, 0.0005, 0.001])\n",
    "            \n",
    "            # Tasa de dropout\n",
    "            dropout_rate = hp.Float(f'dropout_{i}',\n",
    "                                   min_value=0.0,\n",
    "                                   max_value=0.5,\n",
    "                                   step=0.1)\n",
    "            \n",
    "            # Añadir capa Dense\n",
    "            model.add(layers.Dense(\n",
    "                units,\n",
    "                kernel_regularizer=regularizers.l2(reg_rate)\n",
    "            ))\n",
    "            \n",
    "            # Activación\n",
    "            activation = hp.Choice(f'activation_{i}', \n",
    "                                  values=['relu', 'elu', 'selu'])\n",
    "            model.add(layers.Activation(activation))\n",
    "            \n",
    "            # Batch normalization (opcional)\n",
    "            if hp.Boolean(f'batch_norm_{i}'):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            # Dropout (opcional)\n",
    "            if dropout_rate > 0:\n",
    "                model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        # Capa de salida\n",
    "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        # Compilar modelo\n",
    "        learning_rate = hp.Float('learning_rate', \n",
    "                               min_value=1e-4,\n",
    "                               max_value=1e-2,\n",
    "                               sampling='log')\n",
    "        \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, x, y, validation_data, **kwargs):\n",
    "        \"\"\"\n",
    "        Entrena el modelo con early stopping\n",
    "        \"\"\"\n",
    "        batch_size = hp.Int('batch_size', 16, 128, step=16)\n",
    "        \n",
    "        return model.fit(\n",
    "            x, y,\n",
    "            validation_data=validation_data,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=25,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ],\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# Función para obtener tamaño del modelo en KB\n",
    "def get_model_size(model):\n",
    "    \"\"\"Retorna el tamaño del modelo en KB\"\"\"\n",
    "    # Convertir a TFLite para obtener tamaño realista\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    return len(tflite_model) / 1024  # Tamaño en KB\n",
    "\n",
    "# Implementar objetivos personalizados para Keras Tuner\n",
    "class MultiObjectiveNAS(kt.Tuner):\n",
    "    def run_trial(self, trial, x, y, validation_data, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        model = self.hypermodel.build(hp)\n",
    "        \n",
    "        # Entrenar el modelo\n",
    "        history = self.hypermodel.fit(\n",
    "            hp, model, x, y, \n",
    "            validation_data=validation_data,\n",
    "            epochs=100,  # Máximo número de épocas\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Calcular métricas de rendimiento\n",
    "        val_accuracy = max(history.history['val_accuracy'])\n",
    "        \n",
    "        # Calcular tamaño del modelo\n",
    "        model_size = get_model_size(model)\n",
    "        \n",
    "        # Devolver métricas para optimización multiobjetivo\n",
    "        return {\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'model_size': model_size,\n",
    "            'val_loss': min(history.history['val_loss'])\n",
    "        }\n",
    "\n",
    "# Configurar y ejecutar la búsqueda\n",
    "hypermodel = MLPHyperModel(\n",
    "    input_dim=dataset['num_features'], \n",
    "    num_classes=dataset['num_classes']\n",
    ")\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel,\n",
    "    objective=[\n",
    "        kt.Objective('val_accuracy', direction='max'),\n",
    "        kt.Objective('model_size', direction='min')\n",
    "    ],\n",
    "    max_trials=50,\n",
    "    directory='/content/drive/MyDrive/Tesis/Accelerometer_Dataset/nas_results',\n",
    "    project_name='mlp_multiobj_nas'\n",
    ")\n",
    "\n",
    "# Mostrar resumen de la búsqueda\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "tuner.search(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Obtener los mejores modelos según diferentes criterios\n",
    "best_accuracy_model = tuner.get_best_models(1, objective='val_accuracy')[0]\n",
    "best_size_model = tuner.get_best_models(1, objective='model_size')[0]\n",
    "\n",
    "# Obtener frente de Pareto\n",
    "pareto_models = tuner.get_best_models(10)\n",
    "\n",
    "# Visualizar resultados de la búsqueda\n",
    "trials_df = tuner.results_summary(return_dataframe=True)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    trials_df['model_size'], \n",
    "    trials_df['val_accuracy'], \n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Marcar el mejor modelo para cada objetivo\n",
    "plt.scatter(\n",
    "    trials_df['model_size'].min(),\n",
    "    trials_df.loc[trials_df['model_size'].argmin(), 'val_accuracy'],\n",
    "    color='red', marker='*', s=200, label='Modelo más pequeño'\n",
    ")\n",
    "plt.scatter(\n",
    "    trials_df.loc[trials_df['val_accuracy'].argmax(), 'model_size'],\n",
    "    trials_df['val_accuracy'].max(),\n",
    "    color='green', marker='*', s=200, label='Modelo más preciso'\n",
    ")\n",
    "\n",
    "plt.xlabel('Tamaño del modelo (KB)')\n",
    "plt.ylabel('Precisión de validación')\n",
    "plt.title('Frente de Pareto: Tamaño vs Precisión')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar modelo óptimo basado en balance entre precisión y tamaño\n",
    "# Usar técnica de normalización y ponderación\n",
    "normalized_size = (trials_df['model_size'] - trials_df['model_size'].min()) / (trials_df['model_size'].max() - trials_df['model_size'].min())\n",
    "normalized_acc = (trials_df['val_accuracy'] - trials_df['val_accuracy'].min()) / (trials_df['val_accuracy'].max() - trials_df['val_accuracy'].min())\n",
    "\n",
    "# Ponderación: 60% precisión, 40% tamaño\n",
    "weighted_score = 0.6 * normalized_acc - 0.4 * normalized_size\n",
    "best_idx = weighted_score.argmax()\n",
    "\n",
    "print(f\"Modelo óptimo seleccionado:\")\n",
    "print(f\"- Precisión: {trials_df.iloc[best_idx]['val_accuracy']:.4f}\")\n",
    "print(f\"- Tamaño: {trials_df.iloc[best_idx]['model_size']:.2f} KB\")\n",
    "\n",
    "# Obtener y reentrenar el modelo óptimo\n",
    "best_hp = tuner.get_best_hyperparameters(1, objective=weighted_score.name)[0]\n",
    "optimal_model = hypermodel.build(best_hp)\n",
    "optimal_model.summary()\n",
    "\n",
    "# Reentrenar el modelo óptimo con todos los datos de entrenamiento\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=30, \n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/best_nas_mlp.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "optimal_history = optimal_model.fit(\n",
    "    dataset['x_train'], dataset['y_train'],\n",
    "    epochs=200,\n",
    "    batch_size=int(best_hp.get('batch_size')),\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Evaluación del modelo óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo óptimo\n",
    "optimal_test_loss, optimal_test_acc = optimal_model.evaluate(\n",
    "    dataset['x_test'], dataset['y_test']\n",
    ")\n",
    "print(f\"Precisión del modelo óptimo en datos de prueba: {optimal_test_acc:.4f}\")\n",
    "\n",
    "# Obtener predicciones\n",
    "y_pred_proba = optimal_model.predict(dataset['x_test'])\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = dataset['y_test_raw']\n",
    "\n",
    "# Matriz de confusión\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = list(dataset['label_codes_dict'].keys())\n",
    "\n",
    "# Visualizar matriz de confusión normalizada\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matriz de Confusión Normalizada')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Curvas ROC (one-vs-rest para multiclase)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Convertir a one-vs-rest para esta clase\n",
    "    y_true_binary = (y_true == i).astype(int)\n",
    "    y_score = y_pred_proba[:, i]\n",
    "    \n",
    "    # Calcular curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Graficar\n",
    "    plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Azar')\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curvas ROC para cada clase')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# Visualizar ejemplos mal clasificados\n",
    "incorrect_idx = np.where(y_pred != y_true)[0]\n",
    "if len(incorrect_idx) > 0:\n",
    "    plt.figure(figsize=(12, len(incorrect_idx) * 2))\n",
    "    for i, idx in enumerate(incorrect_idx[:10]):  # Mostrar hasta 10 ejemplos incorrectos\n",
    "        x_sample = dataset['x_test'][idx]\n",
    "        true_label = class_names[y_true[idx]]\n",
    "        pred_label = class_names[y_pred[idx]]\n",
    "        \n",
    "        plt.subplot(min(len(incorrect_idx), 10), 1, i+1)\n",
    "        plt.plot(x_sample)\n",
    "        plt.title(f'Verdadero: {true_label}, Predicho: {pred_label}')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 6. Exportación para STM32CubeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo en formato TF\n",
    "optimal_model.save('/content/drive/MyDrive/Tesis/Accelerometer_Dataset/optimal_mlp_model.h5')\n",
    "\n",
    "# Función para convertir a TFLite con diferentes configuraciones\n",
    "def convert_to_tflite(model, quantize=False, optimize=True):\n",
    "    \"\"\"\n",
    "    Convierte el modelo a TFLite con diferentes opciones de optimización\n",
    "    \"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if optimize:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    if quantize:\n",
    "        # Quantize to int8\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        \n",
    "        # Representative dataset generator \n",
    "        def representative_dataset():\n",
    "            for i in range(min(100, len(dataset['x_train']))):  # Usar 100 muestras\n",
    "                sample = dataset['x_train'][i:i+1].astype(np.float32)\n",
    "                yield [sample]\n",
    "                \n",
    "        converter.representative_dataset = representative_dataset\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    return tflite_model\n",
    "\n",
    "# Convertir a diferentes formatos\n",
    "print(\"Convirtiendo modelos para STM32CubeAI...\")\n",
    "\n",
    "# Modelo float32 (no optimizado)\n",
    "tflite_float32 = convert_to_tflite(optimal_model, quantize=False, optimize=False)\n",
    "with open('/content/drive/MyDrive/Tesis/Accelerometer_Dataset/optimal_mlp_float32.tflite', 'wb') as f:\n",
    "    f.write(tflite_float32)\n",
    "print(f\"Modelo float32: {len(tflite_float32)/1024:.2f} KB\")\n",
    "\n",
    "# Modelo float32 optimizado\n",
    "tflite_float32_optimized = convert_to_tflite(optimal_model, quantize=False, optimize=True)\n",
    "with open('/content/drive/MyDrive/Tesis/Accelerometer_Dataset/optimal_mlp_float32_optimized.tflite', 'wb') as f:\n",
    "    f.write(tflite_float32_optimized)\n",
    "print(f\"Modelo float32 optimizado: {len(tflite_float32_optimized)/1024:.2f} KB\")\n",
    "\n",
    "# Modelo int8 (cuantizado)\n",
    "try:\n",
    "    tflite_int8 = convert_to_tflite(optimal_model, quantize=True, optimize=True)\n",
    "    with open('/content/drive/MyDrive/Tesis/Accelerometer_Dataset/optimal_mlp_int8.tflite', 'wb') as f:\n",
    "        f.write(tflite_int8)\n",
    "    print(f\"Modelo int8: {len(tflite_int8)/1024:.2f} KB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en cuantización a int8: {e}\")\n",
    "    print(\"Continuando sin modelo int8...\")\n",
    "\n",
    "# Evaluar modelo TFLite float32\n",
    "def evaluate_tflite_model(tflite_model_path, x_test, y_test):\n",
    "    \"\"\"Evalúa el modelo TFLite en datos de prueba\"\"\"\n",
    "    # Cargar modelo TFLite\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = []\n",
    "    for i in range(len(x_test)):\n",
    "        input_data = np.array(x_test[i:i+1], dtype=np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        y_pred.append(output_data[0])\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calcular precisión\n",
    "    accuracy = np.mean(y_pred_classes == y_true_classes)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Evaluar modelo TFLite float32 optimizado\n",
    "tflite_accuracy = evaluate_tflite_model(\n",
    "    '/content/drive/MyDrive/Tesis/Accelerometer_Dataset/optimal_mlp_float32_optimized.tflite',\n",
    "    dataset['x_test'],\n",
    "    dataset['y_test']\n",
    ")\n",
    "\n",
    "print(f\"\\nComparación de rendimiento:\")\n",
    "print(f\"- Modelo Keras original: {optimal_test_acc:.4f}\")\n",
    "print(f\"- Modelo TFLite optimizado: {tflite_accuracy:.4f}\")\n",
    "print(f\"- Diferencia: {(tflite_accuracy - optimal_test_acc)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nModelos listos para ser usados en STM32CubeAI.\")\n",
    "print(\"Sigue estos pasos para implementar el modelo en tu STM32:\")\n",
    "print(\"1. Abre STM32CubeAI e importa el archivo TFLite (.tflite)\")\n",
    "print(\"2. Genera el código C para tu proyecto STM32\")\n",
    "print(\"3. Implementa las funciones de preprocesamiento necesarias\")\n",
    "print(\"4. Asegura la correcta asignación de memoria para las entradas/salidas del modelo\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
